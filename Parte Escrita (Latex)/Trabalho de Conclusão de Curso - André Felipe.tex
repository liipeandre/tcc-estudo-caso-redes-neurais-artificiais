		% =======================================================================
		% =                                                                     =
		% = ABNTEX - UTP                                                        =
		% =                                                                     =
		% =======================================================================
		% -----------------------------------------------------------------------
		% Author: Chaua Queirolo
		% Data:   01/07/2017
		% -----------------------------------------------------------------------
		\documentclass[12pt,oneside,a4paper,chapter=TITLE,section=TITLE,sumario
		=tradicional]{abntex2}
		
		% Regras da abnt
		\usepackage{packages/abnt-UTP}
		\usepackage{lipsum}
		\usepackage{array}
		\usepackage{tocloft}
		\usepackage{tabularx}
		\usepackage{makecell}
		\usepackage{abntex2cite}
		\usepackage{algpseudocode}
		\usepackage{algorithm}
		\usepackage{listings}
		\usepackage[utf8]{inputenc}
		
		\settocdepth{subsection}
		
		
		\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
		\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
		\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
		
		\renewcommand{\lstlistingname}{Algoritmo}% Listing -> Algorithm
		\renewcommand{\lstlistlistingname}{Lista de \lstlistingname s}% List of Listings -> Lista de Algoritmos
		
		\makeatletter
		\renewcommand{\subparagraph}{%
			\@startsection{paragraph}{4}%
			{\z@}{3.25ex \@plus 1ex \@minus .2ex}{-0.5em}%
			{\normalfont\normalsize\bfseries}%
		}
		\makeatother
		
		% Outros packages (usados durante o trabalho).
		\usepackage[table]{xcolor}
		\usepackage{hyperref}
		\usepackage[utf8]{inputenc}
		\usepackage[bottom]{footmisc}
		\usepackage{verbatim}
		\usepackage{float}
		
		\everymath{\displaystyle}
		\DeclareMathSizes{16}{12}{16}{16}
		% =======================================================================
		% =                                                                     =
		% = DADOS DO TRABALHO                                                   =
		% =                                                                     =
		% =======================================================================
		
		% Informações de dados para CAPA e FOLHA DE ROSTO
		\titulo{Análise de Desempenho de Redes Neurais Artificiais para problemas de classificação}
		
		\autor{André Felipe Pereira dos Santos}
		
		\orientador{Prof. MSc. Chauã Coluene Queirolo Barbosa da Silva.}
		
		\preambulo{Trabalho de Conclusão de Curso apresentado ao curso de Bacharelado 
		em Ciência da Computação da Faculdade de Ciências Exatas e de Tecnologia da 
		Universidade Tuiuti do Paraná, como requisito à obtenção do grau de Bacharel.}
		
		\instituicao{Universidade Tuiuti do Paraná}
		\local{Curitiba}
		\data{2018}
		
		
		% =======================================================================
		% =                                                                     =
		% = DOCUMENTO                                                           =
		% =                                                                     =
		% =======================================================================
		\begin{document}
		
		% evita que as citações estejam em cor azul (hiperlink).
			\hypersetup{%
				colorlinks = true,
				linkcolor  = black,
				allcolors  = black
			}
			
		% evita que as citações de subseções sejam nomeadas como subseção e sim como somente seção.
			
			\let\subsectionautorefname\sectionautorefname
			\let\subsubsectionautorefname\sectionautorefname
		
		% -----------------------------------------------------------------------
		% -                                                                     -
		% - ELEMENTOS PRÉ-TEXTUAIS                                              -
		% -                                                                     -
		% -----------------------------------------------------------------------
		
		% Capa e folha de rosto
		\imprimircapa
		\imprimirfolhaderosto
		
		% Resumo - incluir na versão final

		\begin{resumo}
			
			O conceito de Inteligência Artificial está bastante relacionado à capacidade dos computadores resolverem problemas e executarem funções simples, naturais e solucionáveis por seres humanos. Um dos desafios está em como adaptar tarefas, como o reconhecimento visual, para que a mesma seja realizada por tal, que opera de maneira diferente, e realizá-la com precisão e eficiência. Os conceitos tratados nesse trabalho envolvem não só a área de Visão Computacional, como também as áreas de Reconhecimento Ótico de Caracteres e de Aprendizado de Máquina, com ênfase no uso de Redes Neurais Artificiais~(RNAs). Uma análise de desempenho de uma RNA foi realizada, configurando-a com diferentes parâmetros tais como: taxa de aprendizado, funções de ativação, número de neurônios e de camadas ocultas, visando entender com uma maior clareza o funcionamento de uma RNA, seus parâmetros de configuração e quando utilizá-los, facilitando o uso de tais tecnologias.
					    
			\palavraschave{Aprendizado de Máquina, Processamento Digital de Imagens, Reconhecimento Ótico de Caracteres, Redes Neurais Artificiais, Visão Computacional}    
		\end{resumo}
		
		\begin{resumo}[ABSTRACT]

			The concept of Artificial Intelligence is closely related to the ability of computers to solve problems and perform simple, natural and solvable functions by humans. One of the challenges is how to adapt tasks, such as Visual Recognition, so that it can be performed by such, which operates differently, and perform it accurately and efficiently. The concepts discussed in this work involve not only the area of Computer Vision as well the areas of Optical Character Recognition and Machine Learning, focusing on the use of Artificial Neural Networks~(ANN), where a performance analysis of an ANN was performed, configuring it with different parameters such as: learning rate, activation functions, number of neurons and hidden layers, in order to understand with greater clarity how an ANN operates, its configuration parameters and when to use them, allowing an easier use of such technologies.	
			
			\vspace{0.5cm}
			\textbf{Keywords:\hspace{0.2cm}}{Machine Learning, Digital Image Processing, Optical Character Recognition, Artificial Neural Networks, Computer Vision.} 
		\end{resumo}
			
		% Listas
		\listadefiguras
		%\listadegraficos
		%\listadetabelas
		\listadequadros
		%\listadecodigos
		\listadealgoritmos
		%\lstlistoflistings
		
		% Lista de siglas
		\begin{siglas}
		  \item[ADALINE] 	\textit{Adaptive Linear Neuron}
		  \item[BMVA] 		\textit{British Machine Vision Association}
		  \item[BRIEF] 		\textit{Binary Robust Independent Elementary Features} (Características Binárias Robustas Elementares e Independentes)
		  \item[CMY] 		\textit{Cyan, Magenta, Yellow} (Ciano, Magenta, Amarelo)
		  \item[CMYK] 		\textit{Cyan, Magenta, Yellow, Key} (Ciano, Magenta, Amarelo e Preto)	
		  \item[ELU] 		\textit{Exponential Linear Unit} (Unidade Linear Exponencial)	  
		  \item[FAST] 		\textit{Features from Accelerated Segment Test} (Características de Teste de Segmento Acelerado)
		  \item[HSI] 		\textit{Hue, Saturation, Intensity} (Matiz, Saturação, Intensidade)
		  \item[IA] 		Inteligência Artificial
		  \item[ICA] 		\textit{Independent Components Analysis} (Análise de Componentes Independentes)
		  \item[KNN] 		\textit{K-Nearest Neighbor} (K-Vizinho Mais Próximo)
		  \item[LMS] 		\textit{Least-Mean-Square} (Mínimo Quadrado Médio)
		  \item[M] 			\textit{Momentum}		  
		  \item[MLP] 		\textit{MultiLayer Perceptron} (\textit{Perceptron} Multicamada)
		  \item[OCR] 		\textit{Optical Character Recognition} (Reconhecimento Ótico de Caracteres)
		  \item[ORB] 		\textit{Oriented FAST and Rotated BRIEF} (FAST Orientado e BRIEF Rodado)
		  \item[PCA] 		\textit{Principal Components Analysis} (Análise de Componentes Principais)
		  \item[PReLU] 		\textit{Parametric Rectified Linear Unit} (Unidade Linear Retificada Paramétrica)
		  \item[ReLU] 		\textit{Rectified Linear Unit} (Unidade Linear Retificada)
		  \item[RNA] 		Rede Neural Artificial	  
		  \item[RGB] 		\textit{Red, Green, Blue} (Vermelho, Verde, Azul)
		  \item[SELU] 		\textit{Scaled Exponential Linear Unit} (Unidade Linear Exponencial em Escala)
		  \item[SIFT] 		\textit{Scale Invariant Feature Transform} (Transformação de Características Invariantes de Escala)
		  \item[SURF] 		\textit{Speeded Up Robust Features} (Características Robustas Acelerada)
		  \item[SVM] 		\textit{Support Vector Machine} (Máquinas de Vetor de Suporte)
		\end{siglas}

		% Lista de símbolos
		\begin{simbolos}
		  \item[$\partial$] Derivada parcial
		  \item[$\eta$] Taxa de aprendizado
		  \item[$\varphi$] Função de ativação
		  \item[$\theta$] Ângulo em graus
		  \item[$\varepsilon$] Épsilon
		  \item[$b$] Bias
		  \item[$CMY$] \textit{Pixel} normalizado em CMY
		  \item[$E_{c\hspace{0.1cm}calc}$] Erro da camada atual de cálculo
		  \item[$E_{saida}$] Erro da camada de saída
		  \item[$E_{c\hspace{0.1cm}oculta}$] Erro da camada oculta
		  \item[$F_{ativacao}'$] Derivada primeira da função de ativação
		  \item[$H$] 1ª banda de um \textit{pixel} em HSI
		  \item[$I$] 3ª banda de um \textit{pixel} em HSI
		  \item[$P_{antigo}$] Pesos sinápticos antigos
		  \item[$P_{antigo_{c\hspace{0.1cm}saida}}$] Pesos sinápticos antigos da camada de saída
		  \item[$P_{atualizado}$] Pesos sinápticos atualizados	  
		  \item[$R_{ativacao}$] Resultado da função de ativação
		  \item[$RGB$] Três bandas de um \textit{Pixel} normalizado em RGB 
		  \item[$R_{soma}$] Resultado da função de soma		  
		  \item[$R_{soma_{c\hspace{0.1cm}saida}}$] Resultado da função de soma da camada de saída
		  \item[$S$] 2ª banda de um \textit{pixel} em HSI	
		  \item[$S_{Entrada}$] Sinais de entrada da camada de cálculo
		  \item[$S_{esperada}$] Saída esperada
		  \item[$S_{obtida}$] Saída obtida
		  \item[$w$] Pesos sinápticos
		  \item[$x$] Sinais de entrada ou o resultado da 1ª banda para a normalização do espaço de cor		  
		  \item[$x_{n}$] Sinais de entrada do neurônio
		  \item[$X$] 1ª banda de um \textit{pixel} não normalizado

		  \item[$Xa$] Sinal de entrada $a$
		  \item[$Xb$] Sinal de entrada $b$
		  \item[$y$] Resultado da 2ª banda para a normalização do espaço de cor ou sinal de saída
		  \item[$Y$] 2ª banda de um \textit{pixel} não normalizado
		  \item[$z$] Resultado da 3ª banda para a normalização do espaço de cor
		  \item[$Z$] 3ª banda de um \textit{pixel} não normalizado
		\end{simbolos}
		
		% Sumario		
		\sumario
		\pagebreak		
		
		% -----------------------------------------------------------------------
		% -                                                                     -
		% - ELEMENTOS TEXTUAIS                                                  -
		% -                                                                     -
		% -----------------------------------------------------------------------

		% Inicia a numeracao das páginas
		\textual
		
		\chapter{Introdução}
		\label{cap:introducao}
		
		O Aprendizado de Máquina é uma subárea da Inteligência Artificial~(IA) e seu foco está no aprendizado autônomo a partir do reconhecimento de padrões em uma grande quantidade de dados. Enquanto o cérebro humano tem uma facilidade natural para aprender de forma autônoma a partir dos cinco sentidos~(tato, olfato, paladar, visão e audição), a tarefa de reconhecer padrões em grandes quantidades de dados é feita de forma mais eficiente pelos computadores. 
		
		A tecnologia envolvida no Aprendizado de Máquina tem sido cada vez mais empregada nas grandes corporações para auxiliar nas tomadas de decisões e em muitas outras tarefas do nosso cotidiano.\hspace{0.1cm}Um dos algoritmos pertencentes a essa área são as Redes Neurais Artificias, uma das mais complexas técnicas de Aprendizado de Máquina, criada para simular o aprendizado do cérebro humano em um ambiente artificial, como é o caso do computador. 
		
		As Redes Neurais Artificiais vem sendo cada vez mais aprimoradas, dando origem a inúmeras variações da mesma, onde cada uma possui aplicações distintas, podendo serem aplicadas na solução dos mais diferentes problemas que envolvem raciocínio similar ao do cérebro biológico, problemas também conhecidos como IA-Completo. Alguns exemplos são os problemas que envolvem o processamento de informações e aprendizado a partir da visão humana e da audição, como é o caso das áreas de Visão Computacional e de Processamento de Linguagem  Natural, que buscam dar a mesma capacidade de execução dessas tarefas para os computadores com o auxílio da IA.
		
		Duas dificuldades surgem ao utilizar qualquer técnica de IA:~como utilizar cada uma delas de acordo com o problema proposto, com eficiência e menor esforço, e qual a maneira certa de configurá-las, extraindo o melhor da técnica. Devido a isso, o foco deste trabalho será o desenvolvimento de uma análise de desempenho utilizando RNAs Multicamadas, avaliando a influência de seus parâmetros de configuração na precisão de um problema de classificação, nesse caso, Reconhecimento Ótico de Dígitos escritos à mão que pertencente a área de Visão Computacional, visando auxiliar na solução de outros problemas de classificação que utilizem RNAs. 
				
		Os objetivos específicos deste trabalho são:
		
		\begin{lista}
		    		  
		  \item Determinar o grau de influência das camadas ocultas na precisão da classificação da RNA para o problema de Reconhecimento Ótico de dígitos escritos à mão, considerando o número de camadas ocultas e o número de neurônios em cada uma dessas camadas;
		  
		  \item Determinar o grau de influência da função de ativação em uma RNA na precisão da classificação do problema proposto;
		  
		  \item Determinar o grau de influência da taxa de aprendizado em uma RNA na precisão da classificação do problema proposto;
		  
		  \item Determinar o grau de influência do \textit{momentum} em uma RNA na precisão da classificação do problema proposto;
		  
		  \item Determinar quais dos parâmetros de configuração citados possuem maior influência ao serem utilizados no problema proposto;
		  
		  \item Analisar a variação da precisão da classificação de uma RNA após cada alteração nos parâmetros citados (número de camadas ocultas e número de neurônios em cada camada, função de ativação, taxa de aprendizado e \textit{momentum}).
		  	  
		\end{lista}	
		
		Este trabalho está dividido como segue: 
		
		\begin{lista}
			\item[$\bullet$] O \autoref{cap:fundamentacao-teorica} apresenta a fundamentação dos principais conceitos teóricos utilizados neste trabalho e os trabalhos relacionados;
			
			\item[$\bullet$] O \autoref{cap:metodologia} apresenta a metodologia proposta;
			
			\item[$\bullet$] O \autoref{cap:resultados} apresenta os resultados experimentais;
			
			\item[$\bullet$] O \autoref{cap:conclusao} apresenta as conclusões e os trabalhos futuros.
		\end{lista}
		
		% -----------------------------------------------------------------------
		% -----------------------------------------------------------------------
		\chapter{Fundamentação Teórica}
		\label{cap:fundamentacao-teorica}
		
		Este capítulo aborda os seguintes conceitos: (1)~Reconhecimento Ótico de Caracteres, suas aplicações e dificuldades no reconhecimento; (2)~Visão Computacional, seus conceitos básicos, aplicações, etapas de funcionamento e algumas técnicas de Processamento Digital de Imagens utilizadas em cada etapa; (3)~Aprendizado de Máquina, tipos e técnicas; e, (4) Redes Neurais Artificiais, suas aplicações, funcionamento, parâmetros de configuração e tipos. Além disso, é apresentado o processo de normalização de dados e o funcionamento da biblioteca \textit{scikit-learn}, biblioteca de Aprendizado de Máquina utilizada neste trabalho.
		
		\section{Reconhecimento Ótico de Caracteres}
		
		A área de Reconhecimento Ótico de Caracteres é uma subárea da Visão Computacional, que visa dar a capacidade humana de reconhecer textos, de forma visual, aos computadores.
		
		\begin{citacao}
			"Os sistemas de Reconhecimento Ótico de Caracteres (\textit{Optical Character Recognition} - OCR) são sistemas desenvolvidos para, de uma certa forma, reproduzir a capacidade humana de ler textos"\hspace{0.1cm}\cite{osorio1991estudo}. 
		\end{citacao}
			
		Algumas aplicações do Reconhecimento Ótico de Caracteres incluem:
		
		\begin{lista}
			\item Reconhecimento automático de placas de veículos;
			\item Busca e edição de textos existentes em documentos escaneados;
			\item Conversão de texto manuscrito para Interação Homem-Máquina em tempo-real;
			\item Tecnologias de apoio à deficientes visuais;
			\item Extração de dados de documentos impressos.
		\end{lista}
		
		As dificuldades para o desenvolvimento de sistemas desta natureza dependem dos tipos de caracteres que o mesmo deve reconhecer. Um exemplo é o reconhecimento de caracteres datilografados~(escritos na máquina de escrever) e impressos, onde geralmente o tipo da fonte e o tamanho da letra são padronizados.
		No caso do reconhecimento de caracteres escritos a mão, existe a questão da letra da pessoa e do formato de escrita da mesma, o que dificulta essa tarefa e faz com que a qualidade do processo de segmentação e de extração de características sejam determinantes para seu sucesso. 

  		Alguns exemplos de problemas presentes no reconhecimento de caracteres escritos a mão são vistos na \autoref{fig:prob_rec}.
		
		\begin{figure}[H]
			\legenda[fig:prob_rec]{Problemas no reconhecimento de caracteres escritos à mão}
			\lfig{scale=1.4}{imagens/problemas_reconh1}{Caracteres conectados.}
			\hspace{2cm}
			\lfig{scale=1.4}{imagens/problemas_reconh2}{Inclinação do traço.}	\\
			\lfig{scale=1.4}{imagens/problemas_reconh3}{Traços extras.}
			\hspace{2cm}
			\lfig{scale=1.4}{imagens/problemas_reconh4}{Elementos não numéricos.}	\\
			\lfig{scale=1.4}{imagens/problemas_reconh5}{Inclinação da escrita.}
			\hspace{2cm}
			\lfig{scale=1.4}{imagens/problemas_reconh6}{Caracteres sobrescritos.}
			\fonte{\citeauthoronline{rodrigues2000reconhecimento}, \citeyear{rodrigues2000reconhecimento}}
		\end{figure}
	
		\section{Visão Computacional}
		\label{sec:intro_visi}
		
		O funcionamento de um sistema de reconhecimento de caracteres escritos à mão é bastante semelhante ao funcionamento de sistemas de Visão Computacional. Portanto, deve-se entender o que é Visão Computacional, qual o objetivo da mesma e como os sistemas dessa natureza operam. 
			
		A área de Visão Computacional é composta pela junção de duas áreas de estudo, Processamento Digital de Imagens e Aprendizado de Máquina. A primeira é responsável por permitir a extração de informações úteis a partir de imagens, além de criar técnicas que possibilitam realizar essa tarefa de forma eficiente e precisa. A segunda é responsável por analisar os dados extraídos pela primeira e classificá-los em grupos, isso para que sejam utilizados em tarefas como reconhecer objetos e diferenciar um objeto de outro existente na imagem.
		
		\begin{citacao}
		"A Visão Computacional está preocupada com a extração, análise e compreensão automática de informações úteis a partir de uma única imagem ou sequência de imagens"~\cite{bmvacompvision}.
		\end{citacao}
		
		Algumas áreas e tecnologias que utilizam Visão Computacional são:
		
		\begin{lista}
			\item Carros autônomos~(Figura \ref{fig:carro}).
			
			\begin{figure}[h]
				\legenda[fig:carro]{Carro autônomo da \textit{Google}}
				\fig{scale=0.3}{imagens/carro_autonomo_google}
				\fonte{\citeauthoronline{cesar2016carro}, \citeyear{cesar2016carro}}
			\end{figure}
			
			\item Biomedicina, no diagnóstico de doenças como o câncer~(Figura \ref{fig:diag}).
			
			\begin{figure}[h]
				\legenda[fig:diag]{Processo de divisão celular de uma célula humana de melanoma (câncer de pele)}
				\fig{scale=0.5}{imagens/diag_cancer}
				\fonte{\citeauthoronline{correio2018diag}, \citeyear{correio2018diag}}
			\end{figure}
	
			\item Reconhecimento facial~(Figura \ref{fig:facial}).
			
			\begin{figure}[H]
				\legenda[fig:facial]{Sistema de reconhecimento facial}
				\fig{scale=0.5}{imagens/sist_rec_facial}
				\fonte{\citeauthoronline{galileu2018facial}, \citeyear{galileu2018facial}}
			\end{figure}
		
			\item Reconhecimento de gestos~(Figura \ref{fig:gestos}).
			
			\begin{figure}[H]
				\legenda[fig:gestos]{Sistema de reconhecimento de gestos}
				\fig{scale=0.5}{imagens/sist_rec_gestos}
				\fonte{\citeauthoronline{olhar2018gestos}, \citeyear{olhar2018gestos}}
			\end{figure}
		
			\item Robótica e automação~(Figura \ref{fig:robotica}).
			
			\begin{figure}[H]
				\legenda[fig:robotica]{Robótica e automação}
				\fig{scale=0.19}{imagens/robotica}
				\fonte{\citeauthoronline{elpais2017robotica}, \citeyear{elpais2017robotica}}
			\end{figure}
		
			\item Reconhecimento ótico de caracteres~(Figura \ref{fig:ocr}).
			
			\begin{figure}[H]
				\legenda[fig:ocr]{Sistema de reconhecimento ótico de caracteres}
				\fig{scale=0.75}{imagens/ocr}
				\fonte{\citeauthoronline{ibratan2016ocr}, \citeyear{ibratan2016ocr}}
			\end{figure}
		\end{lista}
		
		\subsection{Etapas de funcionamento de um sistema de Visão Computacional}
		
		O funcionamento de um sistema de Visão Computacional é composto, geralmente, pelas etapas de aquisição da imagem, pré-processamento, segmentação, extração de características e classificação. As etapas descritas são vistas na \autoref{fig:etapas}.
		
		\begin{figure}[H]
			\legenda[fig:etapas]{Etapas de um sistema de Visão Computacional}
			\fig{scale=0.7}{imagens/etapas}
			\fonte{\citeauthoronline{osorio1991estudo}, \citeyear{osorio1991estudo}, p.77, adaptado}
		\end{figure}
		
		\subsubsection{Conceito de imagem}
		
		Um dos conceitos mais básicos e importantes na área de Processamento Digital de Imagem é o conceito de imagem, a qual pode ser descrita como uma matriz bidimensional, onde cada elemento da mesma é chamado de \textit{pixel}. Suas dimensões são chamadas de largura e comprimento, ambas medidas em \textit{pixels}. 		
		
		A representação de um \textit{pixel} varia conforme o modelo de cores (ou espaço de cores) utilizado na imagem. O espaço de cores, segundo~\citeonline[p.264]{gonzalez2010processamento}, serve para facilitar a especificação das cores de uma forma padronizada e amplamente aceita. As formas de representação mais comuns são:
		
		\begin{lista}
			\item Monocromático (ou preto e branco): Cada \textit{pixel} possui um \textit{byte} de tamanho e para a representação da cor utiliza-se apenas os valores 0~(preto) e 255~(branco).
			
			\item Tons de cinza: Cada \textit{pixel} possui um \textit{byte} de tamanho e para a representação da cor, utiliza-se os valores no intervalo de 0~(preto) e 255~(branco)
			 
			\item Colorido: Um dos formatos mais conhecidos é o formato RGB padronizado, também conhecido como cor-luz e cor-aditiva~(ou luminosa), por ser utilizado em fontes de luz. Este formato utiliza três \textit{bytes} para representar um \textit{pixel}, dividido entre três canais de cor: \textit{Red}, \textit{Green}, \textit{Blue}. Cada canal pode assumir valores entre 0~(preto) e 255~(branco), sendo que a combinação dos mesmos compõem a cor. 
		\end{lista}
				
		Outros exemplos de espaços de cor, além do espaço de cores RGB são: CMY~(\textit{Cyan}, \textit{Magenta}, \textit{Yellow}), CMYK~(\textit{Cyan}, \textit{Magenta}, \textit{Yellow}, \textit{Key}) e HSI~(\textit{Hue}, \textit{Saturation}, \textit{Intensity}). Os espaços de cores CMY e CMYK são utilizados em impressoras e enquanto o primeiro utiliza 3 \textit{bytes} para representar um \textit{pixel}, o segundo utiliza 4 \textit{bytes} para o mesmo fim. Ambos são conhecidos como cor-pigmento e cor-subtrativa (ou refletiva) por seu funcionamento ser complementar ao do RGB. Por exemplo, para gerar a cor ciano (cor primária do CMY) em RGB, é necessário adicionar as cores azul e verde~(cores primárias do RGB) para que a cor seja gerada. Já para gerar a cor azul~(cor primária do RGB) em CMY, é necessário adicionar as cores ciano e magenta~(cores primárias do CMY). Considerando isso, o processo de conversão entre CMY e RGB é descrito pela Equação \ref{eq:conversao_cmy}.
			
			\begin{equation}
			\label{eq:conversao_cmy}
			\begin{bmatrix} C\\M\\Y\end{bmatrix} = \begin{bmatrix} 1\\1\\1\end{bmatrix} - \begin{bmatrix} R\\G\\B\end{bmatrix}
			\end{equation}
			
		\hspace{-1.3cm}Onde $[CMY]$ e $[RGB]$ estão normalizados em um intervalo entre 0 e 1. \hspace{-0.05cm}O processo de normalização é descrito pelas Equações \ref{eq:normalizacao_cor}, \ref{eq:normalizacao_cor2} e \ref{eq:normalizacao_cor3}.
			
			\begin{equation}
			\label{eq:normalizacao_cor}
			x = \frac{X}{X+Y+Z}
			\end{equation}
			
			\begin{equation}
			\label{eq:normalizacao_cor2}
			y = \frac{Y}{X+Y+Z}
			\end{equation}
			
			\begin{equation}
			\label{eq:normalizacao_cor3}
			z = \frac{Z}{X+Y+Z}
			\end{equation}

		\hspace{-1.3cm}Onde $X$, $Y$ e $Z$ são os valores de cada banda de um \textit{pixel}, independentemente do espaço de cor utilizado, e a soma entre $x$, $y$ e $z$ deve ser igual a $1$. O CMYK funciona de forma semelhante ao CMY, porém com a adição de uma banda, preta~(K~-~\textit{key}). Tal adição é necessária devido ao fato de não ser possível representá-la no CMY, uma vez que a mistura das três bandas gera um cinza escuro.
			
		O espaço de cores HSI, segundo \citeonline[p.262]{gonzalez2010processamento}, foi criado para facilitar a manipulação de imagens coloridas, isso por representar as cores de forma mais natural e intuitiva à visão humana, dividida em matiz, saturação e intensidade, sendo as duas primeiras responsáveis pela geração da cor em si e a última pela percepção da mesma ser mais ou menos intensa. Utiliza 3 \textit{bytes} para representar um \textit{pixel} e o processo de conversão do RGB para HSI é descrito pelas Equações \ref{eq:conversao_hsi1}, \ref{eq:conversao_hsi2}, \ref{eq:conversao_hsi3} e \ref{eq:conversao_hsi4}.
			
			\begin{equation}
			\label{eq:conversao_hsi1}
			\theta = \arccos\begin{Bmatrix}\frac{\frac{(R-G)+(R-G)}{2}}{[(R-G)^2 + (R-G)(G-B)]^{\frac{1}{2}}}\end{Bmatrix}
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_hsi2}
			H = \begin{Bmatrix}\theta,\hspace{0.1cm} se \hspace{0.1cm} B \leq G  \\ 360-\theta,\hspace{0.1cm} se \hspace{0.1cm} B > G \end{Bmatrix}
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_hsi3}
			S = 1 - \frac{3}{R+G+B} * min(R,G,B)
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_hsi4}
			I = \frac{R+G+B}{3}
			\end{equation}
					
			\hspace{-1.3cm}Onde $R$, $G$ e $B$ são os valores de cada banda de um \textit{pixel} em RGB e $\theta$ o ângulo em graus, utilizado na Equação \ref{eq:conversao_hsi2}.
				
			Já o processo de conversão de HSI para RGB é descrito pelas Equações \ref{eq:conversao_rgb1}, \ref{eq:conversao_rgb2}, \ref{eq:conversao_rgb3},  \ref{eq:conversao_rgb4}, \ref{eq:conversao_rgb5} e \ref{eq:conversao_rgb6}.
			
			\begin{equation}
			\label{eq:conversao_rgb1}
			r = \frac{1 + \frac{S * \cos H}{\cos(60 - H)}}{3}
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_rgb2}
			g = 1 - b - r
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_rgb3}
			b = \frac{1 - S}{3}
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_rgb4}
			R = 3*I*r
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_rgb5}
			G = 3*I*g
			\end{equation}
			
			\begin{equation}
			\label{eq:conversao_rgb6}
			B = 3*I*b
			\end{equation}
								
		\hspace{-1.3cm}Onde $H$, $S$ e $I$ são os valores de cada banda de um \textit{pixel} em HSI.
			
		\subsubsection{Pré-processamento}
		\label{sec:preprocessamento}
		
		Após a etapa de aquisição da imagem, a partir de uma câmera ou outros dispositivos de captura, como equipamentos de radiografia, a imagem capturada é pré-processada visando a remoção de ruídos e a normalização da imagem como um todo. Algumas operações realizadas nessa fase são:
		
		\begin{lista}
			\item Uso de filtros de suavização linear (ou passa-baixa), como os filtros da média, mediana, gaussiano e bilateral. O uso desse tipo de filtro permite a remoção de diversos tipos de ruídos em imagens como os gerados por erros de transmissão de dados e por problemas no momento da captura. O uso desses filtros visam melhorar a qualidade da imagem e facilitando o processamento nas etapas seguintes como pode ser visto nas Figuras \ref{fig:ruido_media}, \ref{fig:ruido_mediana} e \ref{fig:ruido_gauss};
			
			\begin{figure}[H]
				\legenda[fig:ruido_media]{Exemplo de uso do filtro da média}
				\lfig{scale=0.7}{imagens/tipos_ruidos_orig2}{Ruído ``sal e pimenta''.}
				\hspace{0.1cm}
				\lfig{scale=0.7}{imagens/tipos_ruidos_orig}{Ruído gaussiano.} \\  
				\lfig{scale=0.7}{imagens/tipos_ruidos_filt_media}{Imagens (a) e (b) após o uso do filtro da média.}
				\fonte{\citeauthoronline{solomon2011processing}, \citeyear{solomon2011processing}, p.90-97}
			\end{figure}			

			\begin{figure}[H]
				\legenda[fig:ruido_mediana]{Exemplo de uso do filtro da mediana}
				\lfig{scale=0.7}{imagens/tipos_ruidos_orig2}{Ruído ``sal e pimenta''.}
				\hspace{0.1cm}
				\lfig{scale=0.7}{imagens/tipos_ruidos_orig}{Ruído gaussiano.} \\  
				\lfig{scale=0.7}{imagens/tipos_ruidos_filt_mediana}{Imagens (a) e (b) após o uso do filtro da mediana.}			
				\fonte{\citeauthoronline{solomon2011processing}, \citeyear{solomon2011processing}, p.90-97}
			\end{figure}			
					
			\begin{figure}[H]
				\legenda[fig:ruido_gauss]{Exemplo de uso do filtro gaussiano}
				\lfig{scale=0.7}{imagens/tipos_ruidos_orig2}{Ruído ``sal e pimenta''.}
				\hspace{0.1cm}
				\lfig{scale=0.7}{imagens/tipos_ruidos_orig}{Ruído gaussiano.} \\  
				\lfig{scale=0.7}{imagens/tipos_ruidos_filt_gauss}{Imagens (a) e (b) após o uso do filtro gaussiano.}				
				\fonte{\citeauthoronline{solomon2011processing}, \citeyear{solomon2011processing}, p.90-97}
			\end{figure}			
						 
			\item Realce de cor da imagem: O uso dessa técnica permite a melhora na intensidade de cor em regiões da imagem com baixa iluminação;
			
			\item Redimensionamento da imagem: O uso dessa técnica possibilita a diminuição do tamanho da imagem~(em \textit{pixels}), permitindo realizar qualquer operação de forma mais rápida;
			
			\item Conversão da imagem entre espaços de cores: Permite realizar o processamento, utilizando um espaço de cor mais adequado para a tarefa a ser realizada. Por exemplo, converter a imagem do espaço de cor RGB para o HSI facilita a tarefa de realizar o realce da imagem;	
			
			\item Uso da transformada de Fourier: Consiste em transformar a imagem em um sinal de onda~(ou \textit{wavelet}), facilitando o processamento em partes da imagem, isso por permitir que a imagem seja analisada em apenas uma dimensão.
		\end{lista}
			
		\subsubsection{Segmentação}
		\label{sec:segmentacao}
		
		O processo de segmentação consiste em realizar a extração de grupos de \textit{pixels} que representam um objeto na imagem, as chamadas regiões de interesse. As imagens resultantes desse processo são representadas em preto e branco, uma vez que na imagem de entrada são aplicadas máscaras, capturando as regiões existentes na imagem que estão em destaque na máscara (em branco).
		
		\begin{citacao}
		"A segmentação subdivide uma imagem em regiões ou objetos que a compõe.\hspace{0.1cm}O nível de detalhe em que a subdivisão é realizada depende do problema a ser resolvido, ou seja, a segmentação deve parar quando os objetos ou as regiões de interesse de uma aplicação forem detectados"~\cite[p.454]{gonzalez2010processamento}.
		\end{citacao}
	
		\begin{citacao}
		"A segmentação de imagens não é uma tarefa trivial, é uma das tarefas mais difíceis no processamento de imagens.\hspace{0.1cm}A precisão da segmentação determina o sucesso ou o fracasso final dos procedimentos de análise computadorizada"~\cite[p.454]{gonzalez2010processamento}.
		\end{citacao}
	
		Algumas técnicas de segmentação que podem ser utilizadas são descritas a seguir, sendo possível a combinação de duas ou mais delas. Tais técnicas são:
		
		\begin{lista}
			\item Detecção de borda (filtro não-linear ou passa-alta), como os filtros \textit{Sobel}, \textit{Canny} e Laplaciano: Essas técnicas permitem a detecção de regiões de fronteira, extraindo o contorno de cada uma dela para processá-las nas etapas seguintes. Uma imagem exemplificando esse processo pode ser vista na Figura \ref{fig:deteccao_borda}.
						
			\begin{figure}[H]
				\legenda[fig:deteccao_borda]{Aplicação da técnica de detecção de borda}
				\lfig{scale=1.0}{imagens/deteccao_borda_canny}{Imagem de entrada.}
				\lfig{scale=1.0}{imagens/deteccao_borda_canny2}{Imagem gerada pelo filtro \textit{Canny} em (a).}			
				\fonte{\citeauthoronline{gonzalez2010processamento}, \citeyear{gonzalez2010processamento}, p.477}
			\end{figure}			
			
			\item Detecção de cor, textura e/ou movimento: a detecção de cor e textura consiste em localizar regiões a partir da cor e/ou padrão de cor da mesma, definida a partir de um conhecimento prévio sobre o que será extraído da imagem. Já a detecção por movimento consiste em realizar a comparação entre a imagem capturada pela câmera e uma imagem de referência, destacando qualquer região que diferencie. As imagens exemplificando a detecção por cor e por movimento podem ser vistas nas Figuras \ref{fig:deteccao_cor} e \ref{fig:deteccao_mov}. 
			
			\begin{figure}[H]
				\legenda[fig:deteccao_cor]{Exemplo de aplicação da técnica de detecção de cor}
				\lfig{scale=0.5}{imagens/deteccao_mov}{Imagem de entrada.}
				\hspace{2cm}
				\lfig{scale=0.5}{imagens/deteccao_cor}{Máscara gerada após a detecção de cor amarela.} \\
				\lfig{scale=0.5}{imagens/deteccao_cor2}{Imagem resultante da aplicação da máscara (b) em (a).}
					
				\fonte{Próprio autor, 2018}
			\end{figure}
		
			\begin{figure}[H]
				\legenda[fig:deteccao_mov]{Exemplo de aplicação da técnica de detecção de movimento}
				\lfig{scale=0.5}{imagens/deteccao_mov2}{Imagem de entrada.} 
				\hspace{2cm}
				\lfig{scale=0.5}{imagens/deteccao_mov}{Imagem de referência.} \\
				\lfig{scale=0.5}{imagens/deteccao_mov3}{Máscara gerada a partir da diferença entre (a) e (b).}	
				\hspace{2cm}
				\lfig{scale=0.5}{imagens/deteccao_mov4}{Imagem resultante da aplicação da máscara (c) em (a).}				
				\fonte{Próprio autor, 2018}
			\end{figure}		
			
			\item Binarização (ou \textit{thresholding}): Consiste em segmentar a imagem determinando um intervalo para os \textit{pixels} de interesse. A imagem será composta por todos os \textit{pixels} que atendem a esse intervalo. Tal processo pode ser melhor visto na Figura \ref{fig:deteccao_threshold}.
			
			\begin{figure}[H]
				\legenda[fig:deteccao_threshold]{Exemplo de aplicação da técnica de binarização}
				\lfig{scale=1.0}{imagens/deteccao_threshold}{Imagem de entrada.}
				\hspace{2cm}
				\lfig{scale=1.0}{imagens/deteccao_threshold2}{Imagem gerada após a técnica de binarização.}				
				\fonte{\citeauthoronline{solomon2011processing}, \citeyear{solomon2011processing}, p.265}
			\end{figure}
			
			\item Agrupamento de \textit{pixels} (\textit{superpixels}): Consiste em agrupar os \textit{pixels} em grupos maiores, visando diminuir a redundância dos \textit{pixels} em uma imagem e, consequentemente, o esforço para processá-la. Tal processo pode ser visto na Figura \ref{fig:deteccao_agrupamento}.
			
			\begin{figure}[H]
				\legenda[fig:deteccao_agrupamento]{Exemplo de aplicação da técnica de agrupamento de \textit{pixels}}
				\lfig{scale=0.8}{imagens/deteccao_agrupamento}{Imagem de entrada.}
				\hspace{2cm}
				\lfig{scale=0.8}{imagens/deteccao_agrupamento2}{Imagem gerada após a técnica de agrupamento de \textit{pixels} pela média aritmética.}			
				\fonte{\citeauthoronline{kaputr2017computer}, \citeyear{kaputr2017computer}, p.75}
			\end{figure}
			
			\item Divisão e fusão: Consiste em dividir a imagem de forma recursiva conforme a similaridade entre os \textit{pixels}.\hspace{0.1cm}Inicialmente, a imagem é dividida em $n$ conjuntos (ou quadrantes) iguais. Após, é feita a análise de todos os \textit{pixels} de cada um desses conjuntos. Caso todos os \textit{pixels} atendam o critério de similaridade, essa região é rotulada e caso dois ou mais conjuntos possuam similaridades entre eles, esses podem ser unidos. Caso contrário, esse é subdividido igualmente em $n$ subconjuntos (ou quadrantes) e refeita a análise para cada subdivisão.\hspace{0.1cm}Assim como a anterior, essa técnica é bastante eficiente para reduzir o número de \textit{pixels} a serem processados. O processo pode ser visto na Figura \ref{fig:deteccao_divisao}.
			
			\begin{figure}[H]
				\legenda[fig:deteccao_divisao]{Exemplo de aplicação da técnica de divisão e fusão}
				\lfig{scale=0.7}{imagens/deteccao_divisao}{Imagem de entrada.}
				\hspace{1cm}
				\lfig{scale=0.7}{imagens/deteccao_divisao2}{Imagem resultante da técnica de divisão e fusão.}				
				\fonte{\citeauthoronline{solomon2011processing}, \citeyear{solomon2011processing}, p.269}
			\end{figure}
			
			\item \textit{Watershed}: Consiste na visualização de uma imagem em três dimensões, duas coordenadas espaciais e a intensidade, onde as regiões mais escuras~(menor intensidade) possuem maior profundidade que as regiões claras~\cite[p.506]{gonzalez2010processamento}. A partir de então é simulado o processo de inundação, que consiste em preencher a região com ``água'', gerando barragens que se formarão nas regiões mais claras (regiões de borda). O processo pode ser melhor entendido observando a Figura \ref{fig:deteccao_water}.
			
			\begin{figure}[H]
				\legenda[fig:deteccao_water]{Exemplo de aplicação da técnica de \textit{Watershed}}
				\lfig{scale=0.8}{imagens/deteccao_water}{Imagem de entrada.}
				\lfig{scale=0.8}{imagens/deteccao_water2}{Vista topográfica.}
				\lfig{scale=0.8}{imagens/deteccao_water3}{1ª fase da inundação.} \\
				\lfig{scale=0.8}{imagens/deteccao_water4}{2ª fase da inundação.}
				\lfig{scale=0.8}{imagens/deteccao_water5}{Mais inundações.}
				\lfig{scale=0.8}{imagens/deteccao_water6}{Começo da fusão da água de duas \textit{watersheds} (uma barragem foi construída entre elas).} \\
				\lfig{scale=0.8}{imagens/deteccao_water7}{Barragens maiores.}
				\lfig{scale=0.8}{imagens/deteccao_water8}{Imagem final resultante da técnica de \textit{Watershed}.} 		
				\fonte{\citeauthoronline{gonzalez2010processamento}, \citeyear{gonzalez2010processamento}, p.507}
			\end{figure}
			
			\item Rotulação: Aplicada após a geração da máscara, a rotulação consiste em denominar cada região de interesse existente na máscara com uma cor, para então separá-las em imagens distintas pela mesma cor denominada anteriormente. Esse processo pode ser visto na Figura \ref{fig:rotulacao}.
			
			\begin{figure}[h]
				\legenda[fig:rotulacao]{Exemplo de aplicação da técnica de rotulação}
				\lfig{scale=0.9}{imagens/rotulacao}{Imagem de entrada.}
				\hspace{1cm}
				\lfig{scale=0.9}{imagens/rotulacao2}{Rotulação aplicada em (a).} \\
				\lfig{scale=0.9}{imagens/rotulacao3}{1ª máscara resultante após a separação pela cor.}
				\hspace{1cm}
				\lfig{scale=0.9}{imagens/rotulacao4}{2ª máscara resultante após a separação pela cor.}			
				\fonte{Próprio autor, 2018}
			\end{figure}
			
			\item Filtros morfológicos, como por exemplo as operações de erosão, dilatação, abertura e fechamento: Tais filtros são aplicados após o processo de segmentação e são utilizados para realizar correções na imagem, seja preenchendo ou removendo regiões defeituosas. Essas técnicas podem ser vistas nas Figuras \ref{fig:oper_erosao} e \ref{fig:oper_dil}. 
			
			\begin{figure}[H]
				\legenda[fig:oper_erosao]{Exemplo de aplicação da técnica de erosão}
				\lfig{scale=0.9}{imagens/morf_ero}{Imagem de entrada.}
				\hspace{0.3cm}
				\lfig{scale=0.9}{imagens/morf_ero2}{Erosão aplicada em (a) com máscara 11 x 11.} \\
				\lfig{scale=0.9}{imagens/morf_ero3}{Erosão aplicada em (b) com máscara 15 x 15.} 
				\hspace{0.3cm}
				\lfig{scale=0.9}{imagens/morf_ero4}{Erosão aplicada em (a) com máscara 45 x 45.} 	
				\fonte{\citeauthoronline{gonzalez2010processamento}, \citeyear{gonzalez2010processamento}, p.418}
			\end{figure}
		
			\begin{figure}[H]
				\legenda[fig:oper_dil]{Exemplo de aplicação da técnica de dilatação}
				\lfig{scale=1.2}{imagens/morf_dil}{Imagem de entrada.}
				\lfig{scale=1.2}{imagens/morf_dil2}{Imagem resultante do processo de dilatação com máscara 3 x 3.}		
				\fonte{\citeauthoronline{gonzalez2010processamento}, \citeyear{gonzalez2010processamento}, p.419}
			\end{figure}
		\end{lista}
		
		\subsubsection{Extração de características}
		\label{sec:extracao_carac}
		
		Segundo \citeonline[p.522]{gonzalez2010processamento}, nesta fase o agregado de \textit{pixels} segmentados resultante, em geral, é representado e descrito de forma adequada para a futura aplicação da IA. Nessa fase, as imagens não sofrem alterações a serem processadas, já que todos os algoritmos utilizam como dados os \textit{pixels} da imagem de entrada. Alguns algoritmos e técnicas utilizadas nessa etapa são:
		
		\begin{lista}
			\item Detecção de cantos de Harris: O uso dessa técnica permite a extração de cantos em regiões da imagem. Tal algoritmo é bastante utilizado para realizar correlação entre imagens, estabilização de vídeo e modelagem 3D, onde é necessário utilizar as regiões de cantos como referência para o processamento. O resultado da aplicação desse algoritmo pode ser visto na Figura \ref{fig:extracao_harris}.
			
			\begin{figure}[h]
				\legenda[fig:extracao_harris]{Exemplo de aplicação do algoritmo de Harris}
				\fig{scale=0.8}{imagens/extracao_harris}	
				\fonte{\citeauthoronline{solomon2011processing}, \citeyear{solomon2011processing}, p.53}
			\end{figure}
			
			\item SIFT: Essa técnica é uma melhora do algoritmo de Harris, que possui problemas ao detectar cantos em imagens com zoom.\hspace{0.1cm}O seu funcionamento consiste em realizar a mesma análise existente no algoritmo de Harris, considerando também os \textit{pixels} vizinhos dentro de um raio de alcance. Tal técnica serve não só para detecção de cantos, mas também de pontos chaves e é capaz de determinar se um ponto chave é um bom descritor para o objeto existente na imagem.\hspace{0.1cm}Um exemplo de imagem resultante desse processo pode ser vista na Figura \ref{fig:extracao_sift}.
			
			\begin{figure}[h]
				\legenda[fig:extracao_sift]{Exemplo de aplicação do algoritmo SIFT}
				\lfig{scale=0.8}{imagens/extracao_sift2}{Imagem de entrada.} \\
				\lfig{scale=0.76}{imagens/extracao_sift}{Imagem com descritores detectados (círculos).}	
				\fonte{\citeauthoronline{joseph2016opencv}, \citeyear{joseph2016opencv}, p.171}
			\end{figure}
			
			\item SURF: Essa técnica é uma melhoria do SIFT em termos de processamento, pois embora o SIFT seja funcional, o mesmo é bastante lento para ser utilizado em sistemas em tempo-real. Uma imagem resultante desse
			processo pode ser vista na Figura \ref{fig:extracao_surf}.
					
			\item FAST: Essa técnica é uma melhoria do SURF, também em termos de processamento, capaz de detectar potenciais pontos chaves com eficiência. Porém, a mesma não determina se um ponto chave é um bom descritor para o objeto na imagem, ao contrário do uso do SIFT e do SURF. Uma imagem resultante desse
			processo pode ser vista na Figura \ref{fig:extracao_fast}.
				
			\item BRIEF: Essa técnica foi desenvolvida devido aos algoritmos SIFT e SURF serem de código fechado e não poderem ser utilizados comercialmente.\hspace{0.1cm}O algoritmo funciona da mesma maneira que o SURF e o SIFT e é capaz de determinar rapidamente se um ponto chave é um bom descritor para um objeto na imagem, porém não encontra pontos chaves com eficiência. Uma imagem resultante desse processo pode ser vista na Figura \ref{fig:extracao_brief}.
			
			\begin{figure}[h]
				\legenda[fig:extracao_surf]{Imagem com os pontos chaves detectados pelo algoritmo SURF}
				\fig{scale=0.8}{imagens/extracao_surf}
				\fonte{\citeauthoronline{joseph2016opencv}, \citeyear{joseph2016opencv}, p.173}
			\end{figure}
			
			\begin{figure}[h]
				\legenda[fig:extracao_fast]{Exemplo de aplicação do algoritmo FAST}
				\lfig{scale=0.8}{imagens/extracao_fast}{Imagem de entrada}	\\
				\lfig{scale=0.8}{imagens/extracao_fast2}{Imagem com os potenciais pontos chaves detectados (círculos verdes).}	
				\fonte{\citeauthoronline{joseph2016opencv}, \citeyear{joseph2016opencv}, p.173}
			\end{figure}
			
			\begin{figure}[h]
				\legenda[fig:extracao_brief]{Imagem com os pontos chaves detectados pelo algoritmo BRIEF}
				\fig{scale=0.7}{imagens/extracao_brief}
				\fonte{\citeauthoronline{joseph2016opencv}, \citeyear{joseph2016opencv}, p.177}
			\end{figure}
				
			\item ORB: Desenvolvido a partir da junção do FAST com o BRIEF pelo fato de ambos se complementarem. É a solução mais utilizada e uma imagem resultante desse processo pode ser vista na Figura \ref{fig:extracao_orb}.
			
			\begin{figure}[h]
				\legenda[fig:extracao_orb]{Imagem com descritores detectados pelo algoritmo ORB }
				\fig{scale=0.8}{imagens/extracao_orb}
				\fonte{\citeauthoronline{joseph2016opencv}, \citeyear{joseph2016opencv}, p.178}
			\end{figure}
			
			\item PCA: É uma técnica estatística que busca determinar os descritores que possuem menor variação entre duas ou mais imagens de um objeto, considerando como entrada para o cálculo os \textit{pixels} de cada uma das imagens.
						
			\item Transformada de Hough (detecção de formas geométricas): Essa técnica permite a busca de regiões de interesse a partir do formato da mesma.\hspace{0.1cm}Uma imagem resultante desse processo pode ser vista na Figura \ref{fig:extracao_hough}.
			
			\begin{figure}[H]
				\legenda[fig:extracao_hough]{Exemplo de aplicação do algoritmo de Hough}
				\fig{scale=0.7}{imagens/extracao_hough}
				\fonte{\citeauthoronline{opencv2014hough}, \citeyear{opencv2014hough}}
			\end{figure}
		\end{lista}
		
		\section{Inteligência Artificial}
		Na fase de classificação os dados processados na etapa anterior são utilizados para realizar o reconhecimento do objeto extraído, utilizando técnicas de Inteligência Artificial.
		
		A área de Inteligência Artificial divide-se em muitas subáreas, sendo uma delas a de Aprendizado de Máquina, onde seus algoritmos são utilizados em problemas que envolvem o reconhecimento de padrões em grandes quantidades de dados. O principal objetivo da Inteligência Artificial é o desenvolvimento de novas técnicas, visando extrair informações de forma cada vez mais rápida e precisa. Algumas definições de IA são:
		
		\begin{citacao}
		"IA é o estudo de como fazer os computadores realizarem
		coisas que, no momento, as pessoas são
		melhores"~\apud[p.2]{rich1991artificial}{russell2016artificial}.
		\end{citacao}

		\begin{citacao}
		"IA é a automação de atividades que associamos com o pensamento humano, tais como tomadas de decisão, resolução de problemas, aprendizagem, entre outras"~\apud[p.2]{bellman1978introduction}{russell2016artificial}.
		\end{citacao}
		
		\begin{citacao}
		"IA é a capacidade de um computador digital, ou robô controlado por computador, realizar tarefas comumente associadas a seres inteligentes"~\cite{copeland2018ai}.
		\end{citacao}
		
		A área de Aprendizado de Máquina divide-se em dois grupos, a aprendizagem supervisionada e a não-supervisionada.
		
		Segundo~\citeonline[p.35]{joshi2017}, aprendizagem supervisionada se refere ao processo de construir um modelo de Aprendizado de Máquina baseado em dados de treinamento rotulados, enquanto aprendizagem não-supervisionada se refere ao processo de construir um modelo de Aprendizado de Máquina sem os dados de treinamento rotulados. Assim, a diferença entre aprendizagem supervisionada e não-supervisionada está no fato de na primeira existir subgrupos bem definidos, as chamadas classes, sendo que o algoritmo deve apenas aprender a distribuir os dados entre as mesmas, ao contrário da segunda, onde não se sabe quais são as classes e o objetivo é agrupar os dados para defini-las.
		
		 A classificação soluciona o problema de identificar a categoria a qual um conjunto de dados pertence. Alguns algoritmos de aprendizado supervisionado são:

		\begin{lista}
			\item Redes Neurais Artificiais;
			\item K-Nearest Neighbor (KNN);
			\item Support Vector Machine (SVM);
			\item Árvores de decisão;
			\item Classificadores Bayesianos;
			\item Regressão logística;
			\item Regressão linear.
		\end{lista}
		
		Já alguns algoritmos de aprendizado não supervisionado são:
		
		\begin{lista}
			\item K-Means;
			\item Principal Components Analysis (PCA);
			\item Independent Components Analysis (ICA).
		\end{lista}
				
		\subsection{Redes Neurais Artificiais}
		\label{sec:redesneurais}
		
		RNA é uma das técnicas mais complexas pertencentes a área de Aprendizado de Máquina e tem sido cada vez mais utilizada e estudada, visando melhorar a capacidade de simular o raciocínio humano, desenvolvendo novas técnicas de aprendizado e formas de configuração.
		
		\begin{citacao}
			"Uma Rede Neural Artificial (RNA) é um modelo concebido para simular o processo de aprendizagem do cérebro humano. As RNAs são projetadas de modo que possam identificar os padrões subjacentes nos dados e aprender com eles. Podem ser usadas em tarefas como classificação, regressão, segmentação, entre outras"~\cite[p.368]{joshi2017}.
		\end{citacao}
		
		\begin{citacao}
			"Uma RNA é um processador paralelamente distribuído, constituído de unidades de processamento simples, que têm a propensão natural de armazenar conhecimento experimental e torná-lo disponível para uso. As RNAs se assemelham ao cérebro humano em dois aspectos: o conhecimento é adquirido pela rede a partir de seu ambiente através de um processo de aprendizagem e as forças da conexão entre os neurônios, conhecidas como pesos sinápticos, são utilizadas para armazenar o conhecimento adquirido"~\cite[p.1]{haykin2009redes}.
		\end{citacao}
		
		Algumas áreas onde esta tecnologia tem sido empregada são:
		
		\begin{lista}
			\item Processamento de Linguagem Natural;
			\item Visão Computacional;
			\item \textit{Big Data}.		
		\end{lista}
							
		\subsubsection{Representação de um neurônio artificial}
		
		A representação de um neurônio artificial, elemento mais importante dentro de uma RNA pode ser vista na \autoref{fig:neuronio}.
		
		\begin{figure}[H]
			\legenda[fig:neuronio]{Representação de um neurônio artificial}
			\fig{scale=0.7}{imagens/neuronio}
			\fonte{\citeauthoronline{haykin2009redes}, \citeyear{haykin2009redes}, p.12, adaptado}
		\end{figure}
		
		Alguns componentes existentes em cada neurônio artificial são:
		
		\begin{lista}
			\item \textit{Bias}: Segundo \citeonline[p.11]{haykin2009redes}, o \textit{bias} tem o efeito de aumentar ou diminuir a entrada líquida da função de ativação, dependendo se o mesmo for positivo ou negativo.
			
			\item Pesos sinápticos: Os pesos sinápticos são os valores armazenados nas ligações entre dois neurônios. É um dos parâmetros para a função de soma e que são atualizados de acordo com o erro de saída da Rede Neural Artificial, simulando o processo de aprendizado do cérebro humano.
		\end{lista}
		
		Além do \textit{bias} e dos pesos sinápticos, outros componentes existentes em um neurônio artificial são os sinais de entrada, sinais de saída, função de soma e função de ativação.
		
		\subsubsubsection{Sinais de entrada}
		
		Sinais de entrada são valores numéricos utilizados para o cálculo da função de soma~(ver seção \ref{sec:func_soma}). As características extraídas na etapa anterior~(ver \autoref{sec:extracao_carac}) são inseridas na camada de entrada da RNA e são propagadas até a camada de saída da mesma, passando por cada neurônio existente na rede. Dentro de cada neurônio, são aplicadas operações matemáticas que alteram o sinal propagado para os demais neurônios da camada seguinte. 
		
		\subsubsubsection{Sinais de saída}
		
		Os Sinais de saída são os sinais emitidos pela função de ativação e que são transmitidos para os neurônios da camada seguinte, caso exista. Caso contrário, será o resultado do processo de classificação, representado em notação binária, sendo que cada neurônio da camada de saída armazena um \textit{bit} do resultado. Para realizar a comparação entre as classes a serem reconhecidas pela Rede Neural Artificial e os \textit{bits} da camada de saída, cada classe é rotulada com um número em binário, definido pelo arquiteto da RNA.
		
		\subsubsubsection{Função de soma}	
		\label{sec:func_soma}
		A função de soma é responsável por realizar a multiplicação entre todos os sinais de entrada e os seus pesos sinápticos. Esse cálculo é descrito na \autoref{eq:neuronio} e deve ser aplicado para cada neurônio da Rede Neural Artificial.
		
		\begin{equation}
		\label{eq:neuronio}
		R_{soma_{j}} = \sum_{i = 0}^{n}{x_i \cdot w_{ij}}
		\end{equation}
		
		Onde $x_i$ são as entradas, $w_{ij}$ são os pesos sinápticos e $n$ o número de entradas do neurônio.
		
		\subsubsubsection{Função de ativação}	
		
		A função de ativação é também conhecida como função de transferência ou função restritiva e serve para transmitir o sinal elétrico artificial, resultante da função de soma para os demais neurônios conectados a ele, simulando as sinapses presentes no cérebro humano. Além disso, segundo \citeonline[p.37]{haykin2009redes}, restringe o intervalo permissível de amplitude do sinal de saída a um valor finito.
		
		As funções de ativação mais utilizadas e um gráfico com o comportamento de cada uma podem ser vistas a seguir, sendo $R_{ativacao}$ e $R_{soma}$ o resultado da função de ativação e de soma, respectivamente.\hspace{0.1cm}São exemplos de funções de ativação~\cite{reis2016}:
		
		\begin{lista}
			\item Linear: É a mais básica, pois não altera a saída, sendo usada em RNAs de regressão. Seu comportamento é mostrado na Figura \ref{fig:ativ_linear}.
			
			\begin{equation}
			R_{ativacao} = R_{soma}
			\end{equation}
					
			\begin{figure}[H]
				\legenda[fig:ativ_linear]{Comportamento da função linear}
				\fig{scale=1.0}{imagens/linear}
				\fonte{\citeauthoronline{reis2016}, \citeyear{reis2016}}
			\end{figure}
			
			\item[•] Com limite: Define a saída da RNA como binária, de acordo com um limite estabelecido. Seu comportamento é mostrado na Figura \ref{fig:ativ_com_limite}.
			
			\begin{equation}
			R_{ativacao} = \left\{\begin{matrix}
			0,\hspace{0.1cm}se \hspace{0.1cm} R_{soma} < limite\\\hspace{-0.5cm}1,\hspace{0.1cm}caso\ contr\acute{a}rio  
			\end{matrix}\right.
			\end{equation}
			
			\begin{figure}[H]
				\legenda[fig:ativ_com_limite]{Comportamento da função com limite igual a $1$}
				\fig{scale=1.0}{imagens/com_limite}
				\fonte{\citeauthoronline{reis2016}, \citeyear{reis2016}}
			\end{figure}
			
			\item Sigmóide: Também conhecida como função logística, é uma das mais comumente utilizadas por RNAs Multicamadas com propagação positiva, ou \textit{Feedforward}, que precisam ter como saída apenas números positivos. São também utilizadas em outras redes com sinais contínuos. Seu comportamento é mostrado na Figura \ref{fig:ativ_sigmoide}.
			
			\begin{equation}
			R_{ativacao} = \frac{1}{1 + \varepsilon^{-R_{soma}}}
			\end{equation}
			\vspace{0.1cm}
			
			\begin{figure}[H]
				\legenda[fig:ativ_sigmoide]{Comportamento da função sigmóide}
				\fig{scale=1.0}{imagens/sigmoide}
				\fonte{\citeauthoronline{reis2016}, \citeyear{reis2016}}
			\end{figure}
			
			\item Tangente hiperbólica: Utilizada em RNAs que precisam ter saídas com números entre $-1$ e $1$. Seu comportamento é mostrado na Figura \ref{fig:ativ_tan_hiperb}.
			
			\begin{equation}
			R_{ativacao} = \tanh(R_{soma})
			\end{equation}
			
			\begin{figure}[H]
				\legenda[fig:ativ_tan_hiperb]{Comportamento da função da tangente hiperbólica}
				\fig{scale=1.0}{imagens/tan_hiperb}
				\fonte{\citeauthoronline{reis2016}, \citeyear{reis2016}}
			\end{figure}  		
		\end{lista}		
			
		\subsubsection{Topologias de Redes Neurais Artificiais}
		\label{sec:topologias}
		
		Para simular o processo de aprendizagem do cérebro humano, uma Rede Neural Artificial é construída usando camadas compostas por vários neurônios. Alguns exemplos de topologias são as descritas a seguir.
		
		\paragraph{Única camada} 
		
		Também conhecida como \textit{Perceptron}, a RNA de camada única é composta por uma camada de entrada e uma camada de saída, sem camadas ocultas. Na contagem de camadas, a camada de entrada é desconsiderada por não ser realizado cálculos matemáticos na mesma. Uma representação pode ser vista na \autoref{fig:tipos_rede_a}.
		
		\begin{figure}[ht]
			\legenda[fig:tipos_rede_a]{Representação de uma RNA de única camada}
			\fig{scale=1.3}{imagens/rede_neural_unica}\hfil
			\fonte{\citeauthoronline{haykin2009redes}, \citeyear{haykin2009redes}, p.21}
		\end{figure}
			
		\paragraph{Múltiplas camadas}
		
		Também conhecida como \textit{Perceptron} Multicamada (MLP), esse tipo de RNA é composta por uma camada de entrada, pelo menos uma camada oculta e uma camada de saída. Uma representação pode ser vista na \autoref{fig:tipos_rede_b}.
						
		\begin{figure}[H]
			\legenda[fig:tipos_rede_b]{Representação de uma RNA de múltiplas camadas}
			\fig{scale=1.3}{imagens/rede_neural_multi}\hfil
			\fonte{\citeauthoronline{haykin2009redes}, \citeyear{haykin2009redes}, p.22}
		\end{figure}
		
		
		
		\subsubsection{Treinamento de uma Rede Neural Artificial}
		\label{sec:treinamento}
		Segundo \citeonline{silva2001dicas}, para RNAs, existem duas dinâmicas de treinamento que podem ser utilizadas:
		
		\begin{lista}
			\item Por padrão (\textit{on-line} ou incremental), onde os pesos são atualizados após cada amostra de treinamento;
			
			\item Por bloco (\textit{batch}, \textit{epoch} ou lote), onde os pesos são atualizados após cada bloco de treinamento, composto por uma parte das amostras de treinamento da base. 
		\end{lista}
		
		\subsubsection {Normalização da base de treino}	
		\label{sec:padronizacao}
		Todos os algoritmos de Aprendizado de Máquina passam por um  treinamento que consiste em ``ensinar'' o mesmo a classificar dados de acordo com os parâmetros de entrada. O treinamento é realizado utilizando uma base de amostras, onde grande parte da mesma é utilizada no aprendizado dos modelos a serem classificados e o restante é utilizado para testes, onde é avaliada a habilidade do algoritmo em classificar novas amostras.
		
		O grande problema está no fato de que os algoritmos de Aprendizado de Máquina são bastante sensíveis aos dados de treinamento, o que significa que uma dispersão muito grande nos dados de treinamento pode dificultar o aprendizado. Para solucionar isso é necessário que a base de treino passe por um processo de normalização, de forma que a mesma possa ser utilizada na maioria dos algoritmos de Aprendizado de Máquina, com o mínimo de perda de precisão no treinamento. Esse processo diminui a dispersão dos dados pertencentes a cada classe, como pode ser visto na Figura \ref{fig:normalizacao}.
		
		\begin{citacao}
		"O resultado da padronização (ou normalização do escore $Z$) é que os recursos serão redimensionados para que tenham as propriedades padrão de uma distribuição normal, com média igual a zero e desvio padrão igual a um"~\cite{sebastian_normalization}.
		\end{citacao}		
		
		\begin{figure}[h]
			\legenda[fig:normalizacao]{Dados de treinamento transformados após PCA}
			\lfig{scale=1.0}{imagens/normalizacao_b}{Dados não normalizados.}
			\lfig{scale=1.0}{imagens/normalizacao_a}{Dados normalizados.}
			\fonte{\citeauthoronline{vicent_normalization}, \citeyear{vicent_normalization}}
		\end{figure}
			
		\subsubsection{Algoritmos de treinamento de Redes Neurais Artificiais}

		Para cada tipo de RNA citado na seção \ref{sec:topologias}, o algoritmo utilizado para o ajuste dos pesos sinápticos é diferente. O ajuste dos pesos sinápticos é necessário para corrigir a classificação, caso a RNA realize a mesma de forma incorreta, simulando assim o aprendizado. O funcionamento de cada um dos algoritmos é descrito a seguir.
		
		\subsubsubsection{Algoritmo do Mínimo Quadrado Médio (LMS)}
		\label{sec:alg_lms}
		
		Desenvolvido para ser aplicado no primeiro modelo de Rede Neural Artificial, o \textit{Perceptron} (ver \autoref{fig:tipos_rede_a}), esse algoritmo é capaz de aprender a classificar dados de forma binária (em duas classes de dados), sendo aplicado somente em problemas linearmente separáveis (quando é possível encontrar uma reta que separe os dados em dois grupos). 
				
		O passo a passo do algoritmo LMS é descrito a seguir.
		
		\begin{lista}
			\item[1.] Realiza-se o cálculo da função de soma para todos os neurônios da camada de saída.
			
			\item[2.] Realiza-se o cálculo da função de ativação para todos os neurônios da camada de saída e transmite-se o resultado para a saída.
			
			\item[3.] Calcula-se o erro para cada saída da Rede Neural Artificial, utilizando a Equação \ref{eq:calculo_erro}.
			
			\begin{equation}
			\label{eq:calculo_erro}
			E_{saida} = S_{esperada} - S_{obtida}
			\end{equation}	
			
			Onde $E_{saida}$ é o erro da camada de saída, $S_{esperada}$ é a saída esperada na camada de saída, definida na criação da base de treinamento, e $S_{obtida}$ é a saída obtida na mesma, resultado da função de ativação da camada de saída.	
			
			\item[4.]Por último, é realizado o processo de ajuste corretivo dos pesos sinápticos. Esse processo consiste em aplicar um cálculo matemático em todas as ligações entre dois neurônios da Rede Neural Artificial. Um grande problema existente nesse processo está na dificuldade de determinar qual a quantidade ideal de ajustes nos pesos sinápticos para que o erro calculado no passo anterior seja mínimo. Existem duas soluções para o problema:
			
			\begin{lista}
				\item Força bruta: Testa-se todas as combinações possíveis de valores para cada um dos pesos sinápticos, até que o erro mínimo seja alcançado. Extremamente lento e não utilizado;
				
				\item Estimativa: Utiliza-se uma função de custo para computar o erro, considerando o valor atual de todos os pesos sinápticos da RNA. A função de custo mais comum é a função do mínimo quadrado médio, descrita na Equação \ref{eq:funcao_minimo}.
								
				\begin{equation}
				\label{eq:funcao_minimo}
				E_{pesos_{camada}} = \sum{\frac{ {E_{saida}}^2}{2}}
				\end{equation}
				
				Onde $E_{pesos_{camada}}$ é o erro existente em todos os pesos sinápticos de uma camada da RNA. Por ser uma RNA de camada única, o $E_{pesos_{camada}}$ é igual ao erro global da RNA, que representa o erro de todos os pesos sinápticos existentes na mesma e que deve ser minimizado. $E_{saida}$ é o erro da camada de saída, resultado da Equação \ref{eq:calculo_erro}.
				
				Para cada peso sináptico, é aplicada o conceito da descida de gradiente, onde através da Equação \ref{eq:calc_desc_grad}, é possível determinar se a função de custo está subindo ou descendo para esse peso sináptico. Isso indica que deve-se incrementar sucessivamente o peso sináptico $w_{i_{j}}$ enquanto o $Gradiente$, resultado da Equação \ref{eq:calc_desc_grad}, diminuir.
				
				\begin{equation}
				\label{eq:calc_desc_grad}
				Gradiente = \frac{\partial E_{pesos_{camada}}}{\partial w_{i_{j}}}
				\end{equation}
				
				Onde $\partial E_{pesos_{camada}}$ é a derivada parcial do erro de todos os pesos sinápticos e $\partial w_{i_{j}}$ é a derivada parcial do peso sináptico a ser atualizado.
				
			\end{lista}
			
			A Equação \ref{eq:atualiza_pesos_a} representa a equação definitiva para a atualização dos pesos sinápticos.
				
			\begin{equation}
			\label{eq:atualiza_pesos_a}
			P_{atualizado}\ = P_{antigo}\ -\ \eta\ * Gradiente
			\end{equation}	
			
			Onde $P_{atualizado}$ e $P_{antigo}$ são os pesos sinápticos atualizados e antigos, $\eta$ é a taxa de aprendizagem, uma constante utilizada na correção dos pesos sinápticos que determina o quanto a Rede Neural Artificial vai ``aprender'' de cada amostra de treinamento e o $Gradiente$, resultado da Equação \ref{eq:calc_desc_grad}.
			Segundo \citeonline[p.103]{haykin2009redes}, o gradiente para RNAs de camada única pode ser descrito por $x_{n} * $
		\end{lista}			
		
		\subsubsubsection{Algoritmo de Retropropagação (\textit{Backpropagation})}
		\label{sec:alg_backprop}
		
		O \textit{Backpropagation} é desenvolvido para o treinamento de RNAs de múltiplas camadas (ver \autoref{fig:tipos_rede_b}). Foi criado com base no algoritmo do Mínimo Quadrado Médio (LMS), corrigindo o problema da operação XOR. O problema do XOR se resume ao fato de não ser possível traçar uma única reta para separar os dados em dois conjuntos, como pode ser visto na \autoref{fig:prob_xor}, onde $Xa$ e $Xb$ são entradas binárias e $y$ a saída.
		
		\begin{figure}[h]
			\legenda[fig:prob_xor]{Problema do XOR}
			\fig{scale=0.85}{imagens/problema_xor}
			\fonte{\citeauthoronline{haykin2009redes}, \citeyear{haykin2009redes}, p.144}
		\end{figure}
				
		Segundo \citeonline{haykin2009redes}, o algoritmo de retropropagação consiste de dois passos através das diferentes camadas da rede. No passo para frente (propagação), o funcionamento é semelhante aos passos 1, 2 e 3 do algoritmo do Mínimo Quadrado Médio. Já no passo para trás (retropropagação ou \textit{backpropagation}), os pesos sinápticos são ajustados conforme uma regra de correção de erro, sendo semelhante ao passo 4 do algoritmo do Mínimo Quadrado Médio.
		
		Para o correto funcionamento deste algoritmo, alguns ajustes foram feitos no anterior para que o mesmo fosse funcional com a existência de camadas intermediárias. O passo a passo do algoritmo de retropropagação para uma RNA de múltiplas camadas é descrito a seguir.
		
		\begin{lista}
			\item[1.] Realiza-se o cálculo da função de soma para os neurônios da 1ª camada oculta.
			
			\item[2.] Realiza-se o cálculo da função de ativação para todos os neurônios da 1ª camada oculta e transmite o resultado para camada seguinte.
			
			\item[3.] Repete-se os passos 1 e 2 para as camadas seguintes da RNA, até que todas sejam computadas.
			
			\item[4.] Calcula-se o erro para cada saída da Rede Neural Artificial, utilizando a Equação \ref{eq:calculo_erro}, onde $E_{saida}$ é o erro da camada de saída, $S_{esperada}$ é a saída esperada na camada de saída, definida na criação da base de treinamento, e $S_{obtida}$ é a saída obtida na mesma, resultado da função de ativação da camada de saída.	
			
			\item[5.] Nas etapas seguintes, assim como no algoritmo do mínimo quadrado médio, é necessário utilizar a função de custo para computar o erro de cada peso sinápticos, considerando o valor atual de todos os pesos sinápticos da RNA. Como essa é uma RNA de múltiplas camadas, o processo de descida de gradiente deve ser adaptado para RNAs multicamadas.
			
			\begin{equation}
			\label{eq:funcao_minimo2}
			E_{pesos_{camada}} = \sum{\frac{ {E_{saida}}^2}{2}}
			\end{equation}
			
			A partir da função do mínimo quadrado médio,  substituindo $E_{saida}$ por $S_{esperada} - S_{obtida}$, adicionando $\partial{w_{i_{j}}}$ no denominador, a equação se transforma na Equação \ref{eq:calc_desc_grad}.
			
			\begin{equation}
			\label{eq:funcao_minimo3}
			Gradiente = \frac{\sum{\partial{\frac{ {(S_{esperada} - S_{obtida})}^2}{2}}}}{\partial{w_{i_{j}}}}
			\end{equation}
			
			Resolve-se a duas derivadas no numerador e denominador da fração, utilizando a regra da potência, e ignorando temporariamente o somatório.
			
			\begin{equation}
			\label{eq:funcao_minimo4}
			Gradiente = S_{esperada} - S_{obtida}
			\end{equation}
			
			Aplica-se a regra da cadeia, conhecida também como o produto das derivadas. Como $S_{esperada}$ não muda os pesos sinápticos, sua derivada é igual a zero. Desse processo se obtém o gradiente para a camada de saída.
			
			\begin{equation}
			\label{eq:funcao_minimo4}
			Gradiente = (S_{esperada} - S_{obtida}) * \frac{\partial{S_{obtida}}}{\partial{w_{i_{j}}}}
			\end{equation}
			
			Aplicando-se a regra da cadeia novamente, se obtém o gradiente para a última camada oculta.
			
			\begin{equation}
			\label{eq:funcao_minimo4}
			Gradiente = (S_{esperada} - S_{obtida}) * \frac{\partial{S_{obtida}}}{\partial{R_{soma_{saida}}}} * \frac{\partial{R_{soma_{camada}}}}{\partial{w_{i_{j}}}}
			\end{equation}
			
			Aplicando recursivamente a regra da cadeia, expandindo-a, permite encontrar a equação que gera o $Gradiente$ para as próximas camadas da RNA. Isso é necessário devido da equação do $Gradiente$ variar conforme o número de camadas que existem antes dela, contando a partir da camada de saída. A Equação \ref{eq:atualiza_pesos_b} representa a equação definitiva para a atualização dos pesos sinápticos.
			
			\begin{equation}
			\label{eq:atualiza_pesos_b}
			P_{atualizado}\ = P_{antigo}\ -\ \eta\ * Gradiente
			\end{equation}	
			
			Onde $P_{atualizado}$ e $P_{antigo}$ são os pesos sinápticos atualizados e antigos, $\eta$ é a taxa de aprendizagem e o $Gradiente$ é o resultado das equações geradas após cada aplicação da regra da cadeia, uma para cada camada da RNA. O gradiente funciona da mesma maneira que no algoritmo anterior, ou seja, indica que deve-se incrementar sucessivamente o peso sináptico $w_{i_{j}}$ enquanto o $Gradiente$, resultado da equação gerada pela regra da cadeia, diminua.
			
		\end{lista}
		
		\subsubsection{Constantes de configuração}
		\label{sec:constantes_config}
		Além dos tipos de algoritmos de correção de erro, existem algumas constantes que são utilizadas dentro do algoritmo e na definição da topologia de uma Rede Neural Artificial, as quais influenciam na eficiência do treinamento. São elas:
		
		\begin{lista}
			\item Taxa de aprendizagem: Segundo \citeonline{silva2001dicas}, um valor muito baixo torna o aprendizado da rede muito lento e um valor muito alto causa oscilações no treinamento. A taxa de aprendizado tem seu valor variando entre $0$ e $1$, não sendo possível assumir o valor $0$ por impedir que a RNA aprenda. Uma sugestão para esse parâmetro é o valor $0,4$.
			
			\item \textit{Momentum}: O \textit{momentum} é uma constante, assim como a taxa de aprendizagem, e tem como objetivo aumentar a velocidade do treinamento e reduzir o perigo de instabilidade do mesmo. Segundo \citeonline{silva2001dicas}, esse parâmetro é opcional e, assim como a taxa de aprendizado, seu valor varia entre $0$ e $1$, sendo o valor recomendado $0,3$. Existe também um cálculo melhorado do \textit{momentum} chamado de \textit{momentum} de \textit{Nesterov}, existente na maioria das bibliotecas de Aprendizado de Máquina.
				
			\item Número de camadas ocultas: Segundo \citeonline{silva2001dicas}, muitas camadas ocultas fazem com que o erro global da Rede Neural Artificial aumente devido à execução do algoritmo de Retropropagação~(\textit{Backpropagation}), prejudicando na precisão da classificação. Por isso, os autores sugerem que o número de camadas ocultas seja de uma ou no máximo duas.
				
			\item Número de ciclos: Segundo \citeonline{silva2001dicas}, um número excessivo de ciclos pode levar a rede à perda do poder de generalização (\textit{overfitting}). Por outro lado, com um pequeno número de ciclos a rede pode não chegar ao seu melhor desempenho (\textit{underfitting}). Os autores sugerem um valor entre 500 e 3000 ciclos de treinamento. É necessário deixar claro que um número de ciclos muito elevado também eleva significativamente o tempo de treinamento.
				
			\item Número de neurônios na camada oculta: O número de neurônios na camada oculta é determinado através de testes. Segundo \citeonline{silva2001dicas}, um número muito grande de neurônios faz com que o fenômeno de \textit{overfitting} ocorra, o qual acontece quando uma Rede Neural Artificial ``decora'' os modelos utilizados na etapa de treinamento, fazendo com que a mesma somente consiga classificar as instâncias já utilizadas, isso devido ao modelo possuir uma margem de erro muito pequena. 
			
			O uso de um número pequeno de neurônios força o algoritmo a gastar mais tempo procurando uma solução ótima, podendo limitar a mesma em um mínimo local e não ideal. Esse fenômeno, conhecido como \textit{underfitting}, acontece quando uma RNA não consegue ``aprender'' os modelos utilizados na etapa de treinamento devido à limitação da topologia da rede e da margem de erro muito grande da mesma. Os fenômenos de \textit{underfitting} e \textit{overfitting} podem ser melhor entendidos observando a Figura \ref{fig:underfitting}. 
						
			\begin{figure}[H]
				\legenda[fig:underfitting]{\textit{Underfitting} e \textit{overfitting}}
				\lfig{scale=0.6}{imagens/underfitting}{\textit{Underfitting.}}
				\hspace{0.3cm}	    
				\lfig{scale=0.6}{imagens/fitting_ideal}{\textit{Fitting} ideal.}
				\hspace{0.3cm}
				\lfig{scale=0.6}{imagens/overfitting}{\textit{Overfitting}.}
				\fonte{\citeauthoronline{vitalflux}, \citeyear{vitalflux}}
			\end{figure}
		
			\item \textit{Alpha}: Esse parâmetro serve como um regularizador dos pesos sinápticos, penalizando pesos sinápticos com valores muito altos e tem como objetivo evitar o \textit{overfitting}.
		
		\end{lista}
		
		\subsection{Biblioteca \textit{scikit-learn}}
		
		Por ser desenvolvida utilizando o paradigma orientado a objetos, o funcionamento da biblioteca \textit{scikit-learn} é simples.\hspace{0.1cm}Cada um dos algoritmos de Inteligência Artificial é representado como uma classe, que para serem utilizadas dentro de uma aplicação é necessário criar um objeto, chamando o construtor dessa classe e passando, como parâmetros as configurações necessárias pelo algoritmo, como é apresentado no Algoritmo \ref{alg:declaracao_scikit}.
		
		\begin{algoritmo}[H]
			\legenda[alg:declaracao_scikit]{Declarando um algoritmo de Inteligência Artificial no \textit{scikit-learn}}
			\begin{lstlisting}[language=Python, literate={{â}{{\^a}}1{á}{{\'a}}1{à}{{\`a}}1{ã}{{\~a}}1{é}{{\'e}}1{ê}{{\^e}}1{í}{{\'i}}1{ó}{{\'o}}1{õ}{{\~o}}1{ú}{{\'u}}1{ü}{{\"u}}1{ç}{{\c{c}}}1}]
			# Importando módulos da biblioteca scikit-learn (sklearn)
			from sklearn.neural_network import MLPClassifier		
			from sklearn.neighbors import NearestNeighbors
			from sklearn.svm import SVC
			
			# Declarando exemplos de algoritmos de IA. 
			# Cada algoritmo possui parâmetros próprios.
			# (parametro1, parametro2,...)
			
			# Rede Neural Artificial Multicamada (MLP)
			rede_neural = MLPClassifier(parametro1, parametro2,...)
			
			# KNN
			knn = NearestNeighbors(parametro1, parametro2,...)
			
			# Support Vector Machine
			svm = SVC(parametro1, parametro2,...)\end{lstlisting}
			\fonte{Próprio autor, 2018}
		\end{algoritmo}
		
		Para cada algoritmo suportado pela biblioteca, as configurações necessárias são distintas. Alguns parâmetros de configuração para Redes Neurais Artificiais são descritos na seção \ref{sec:constantes_config}. Cada algoritmo também possui suas próprias operações, porém algumas são comuns a maioria deles como é apresentado no Algoritmo \ref{alg:operacoes_scikit}.
		
		\begin{algoritmo}[H]
			\legenda[alg:operacoes_scikit]{Chamando operações de um algoritmo de Inteligência Artificial no \textit{scikit-learn}}
			\begin{lstlisting}[language=Python, literate={{â}{{\^a}}1{á}{{\'a}}1{à}{{\`a}}1{ã}{{\~a}}1{é}{{\'e}}1{ê}{{\^e}}1{í}{{\'i}}1{ó}{{\'o}}1{õ}{{\~o}}1{ú}{{\'u}}1{ü}{{\"u}}1{ç}{{\c{c}}}1}]		
			# Carregando a base de treinamento
			
			# Considere CarregarDataset(NomeArquivo) 
			# como uma função que carrega dados de um arquivo e 
			# retorna os dados armazenados no mesmo, formatados e em
			# uma variável em memória
			entrada, saida_esperada = CarregarDataset(NomeArquivo)
			
			# --------------------------------------------------
			# Principais operações:
			# Sintaxe: objeto.operacao(parametro1, parametro2,...)
			# --------------------------------------------------
			
			# -------------------------------------------------
			# fit(): Realiza o treinamento do modelo com dados
			# 	Entrada: entrada e saida_esperada
			# 	Saída: nenhum	
			# -------------------------------------------------	
			rede_neural.fit(entrada, saida_esperada)
			
			# -------------------------------------------------
			# score(): Avalia o treinamento do modelo com dados
			# 	Entrada: entrada_t e saida_esperada_t
			# 	Saída: porcentagem de acerto
			# -------------------------------------------------
			
			# Carregando a base de teste
			entrada_t, saida_esperada_t = CarregarDataset(NomeArquivo)	
			precisao = rede_neural.score(entrada_t, saida_esperada_t)
			
			# ----------------------------------------------------
			# predict(): Classifica um dado e é utilizado dentro 
			# 			     da aplicação final
			# 	Entrada: dado não classificado (dado_para_classificar)
			# 	Saída: classe do dado
			# ----------------------------------------------------	
			dado_para_classificar = [[0, 1, 2, 3]]
			classe = rede_neural.predict(dado_para_classificar)
			\end{lstlisting}
			\fonte{Próprio autor, 2018}
		\end{algoritmo}
		
		O processo de normalização também pode ser feito através do \textit{scikit-learn} como é apresentado no Algoritmo \ref{alg:normalizacao_scikit}.
		
		\begin{algoritmo}[H]
			\legenda[alg:normalizacao_scikit]{Processo de normalização dos dados de entrada no \textit{scikit-learn}}
			\begin{lstlisting}[language=Python, literate={{â}{{\^a}}1{á}{{\'a}}1{à}{{\`a}}1{ã}{{\~a}}1{é}{{\'e}}1{ê}{{\^e}}1{í}{{\'i}}1{ó}{{\'o}}1{õ}{{\~o}}1{ú}{{\'u}}1{ü}{{\"u}}1{ç}{{\c{c}}}1}]		
			# Importando módulos da biblioteca scikit-learn (sklearn)
			from sklearn.neural_network import MLPClassifier		
			from sklearn.preprocessing import StandardScaler
			
			# Carregando a base de treinamento
			entrada, saida_esperada = CarregarDataset(NomeArquivo)
			
			# Declarando o modelo normalizador
			scaler = StandardScaler()
			
			# Treinando o modelo normalizador com a base de treinamento
			scaler.fit(entrada)
			
			# Para realizar a normalização dos dados 
			# deve-se chamar a operação transform()
			
			# A normalizacao deve feita com qualquer dado de entrada
			# a ser utilizado pelo algoritmo de IA, uma vez que
			# nesse processo ignora-se a saída esperada
			entrada = scaler.transform(entrada)
			
			# Criando o algoritmo de IA como os seus parâmetros
			rede_neural = MLPClassifier(parametro1, parametro2,...)
			
			# Treinando-o com os dados normalizados
			rede_neural.fit(entrada, saida_esperada)
			
			# Para classificar um dado, deve-se primeiro passá-lo para
			# o modelo normalizador
			dado_para_classificar = [[0, 1, 2, 3]]
			dado_para_classificar = scaler.transform(dado_para_classificar)
			
			# Classificando o dado
			classe = rede_neural.predict(dado_para_classificar)\end{lstlisting}
			\fonte{Próprio autor, 2018}
		\end{algoritmo}
		
		Na maioria das vezes, os modelos treinados pela função \texttt{fit()}, como os algoritmos de IA, e o \texttt{StandardScaler()} são armazenados em arquivos que serão utilizados pela aplicação final, sem a necessidade de treinar o modelo novamente. 
		
		\section{Trabalhos Relacionados}
		\label{cap:trabalhos}
		
		Esta seção apresenta uma relação de trabalhos relacionados com o problema de reconhecimento ótico de caracteres, destacando as principais estratégias empregadas pelos autores. Ao final da seção é apresentado um quadro comparativo dos trabalhos.
		
		Garris~\textit{et al.}~(1997) desenvolveram um sistema de reconhecimento de caracteres para formulários escritos a mão. A partir de uma imagem do formulário preenchido, o sistema utiliza técnicas de processamento de imagens para, inicialmente, extrair os dados existentes em cada campo do mesmo e processá-los para extrair cada letra escrita à mão daquele campo. Por último, é utilizada uma Rede Neural Artificial para o reconhecimento da letra, cujo resultado é armazenado em um arquivo em disco. A base de treino foi composta por 3.669 formulários de pessoas distintas, que somados geraram 667.758 caracteres, divididos entre dígitos de 0 a 9 e letras maiúsculas e minúsculas. Como resultado, os autores obtiveram precisão média de 95\% para o reconhecimento de dígitos, 90\% para o reconhecimento de letras maiúsculas e 80\% para o reconhecimento de letras minúsculas.  
		
		Cohen~\textit{et al.}~(2017) desenvolveram um \textit{dataset} variante derivada do \textit{NIST Special Database 19}, nomeado \textit{Extended MNIST (EMNIST)}, com as mesmas classes de reconhecimento do NIST (dígitos, letras maiúsculas e minúsculas), porém com mais características extraídas de cada letra e mais amostras de treino e testes. Tal \textit{dataset} foi utilizado para realizar um comparativo de desempenho entre um classificador linear e o \textit{Extreme Learning Machine} (ELM), utilizando a técnica de treinamento de uma Rede Neural Artificial conhecida como \textit{Online Pseudo-Inverse Update Method} (OPIUM) a qual permite que se trabalhe com \textit{datasets} de qualquer tamanho. Para fazer a análise, os autores utilizaram técnicas de Processamento Digital de Imagens para realizar a conversão das imagens que foram capturadas com as dimensões de 128 \textit{pixels} de altura por 128 \textit{pixels} de largura para imagens com 28 \textit{pixels} de altura por 28 \textit{pixels} de largura, isso para que fossem compatíveis com o sistema original desenvolvido por Garris~\textit{et al.}~(1997) e com o \textit{dataset} MNIST, desenvolvido anteriormente a partir da união de dois \textit{datasets} do NIST original. Como resultado, a Rede Neural Artificial ELM obteve um desempenho médio entre 10\% e 20\% melhor do que o classificador linear.		
		
		\citeonline{gomes2014reconhecimento} desenvolveram um aplicativo para \textit{Android} que, a partir de uma imagem da câmera de um \textit{smartphone}, reconheça placas de velocidade máxima permitida em uma via. Para isso, utilizaram técnicas de Processamento Digital de Imagens para extrair, se existente, a placa da via e a numeração da mesma, que indica a velocidade permitida. Na detecção das placas, o software desenvolvido obteve 45,3\% de precisão, utilizando um banco de dados com 12.520 imagens com placas em diferentes inclinações e distâncias. Já para a etapa de reconhecimento, os autores utilizaram uma Rede Neural Artificial multicamada para classificar as placas em 5 classes distintas, 20, 30, 40, 60 e 80km/h, e obtiveram uma precisão de aproximadamente 96,6\% utilizando as 5.677 imagens, onde a detecção da placa foi feita corretamente.  
		
		\citeonline{osorio1991estudo} desenvolveu um sistema de reconhecimento de caracteres impressos de múltiplas fontes (datilografados, impressos, textos de revistas e de livros e também escritos em letra de fôrma). Para isso, foram utilizadas técnicas de Processamento Digital de Imagens para, inicialmente, extrair regiões com texto e, após, extrair as letras que as compõem. Por último, utilizou-se uma Rede Neural Artificial nomeada pelo autor de ADAn-LIeNE, a qual se adapta, adicionando ou removendo neurônios conforme os dados de entrada, durante o treinamento de cada classe, sendo esse o princípio do comportamento de uma Rede Neural Artificial \textit{Adaptive Resonance Theory} (ART). O banco de dados foi composto por 34 classes de caracteres de A até Z, excluindo as letras O e I, e dígitos de 0 até 9, onde 0 e 1 não são distinguidos de O e I, das diferentes fontes citadas, obtendo um desempenho próximo de 100\% no reconhecimento.
		
		\citeonline{ferreira2012reconhecimento} desenvolveram um sistema de reconhecimento automático de placas veiculares. Para isso, utilizando técnicas de Processamento Digital de Imagens, inicialmente foi detectada a localização da placa através da cor da mesma, juntamente com o seu posicionamento na frente do veículo. Após, corrigiu-se a inclinação da placa e extraiu-se o conteúdo existente na placa do veículo. Por último, extraíram-se cada número e letra existente na placa para reconhecimento utilizando uma Rede Neural Artificial de 750 entradas, 90 neurônios na camada oculta e 26 saídas, além de várias amostras de letras e números para realizar o treinamento. Como resultado, obtiveram uma precisão de praticamente 100\% no reconhecimento dos caracteres.
		
		\citeonline{jan2016optical} desenvolveram um sistema de reconhecimento de caracteres em \textit{Saraiki} (língua nativa do Paquistão e que tem natureza cursiva, mesmo no computador). Para isso, utilizaram técnicas de Processamento Digital de Imagens para realizar a extração de texto de uma imagem e, após, extrair cada letra contida nesse texto. Cada letra obtida foi utilizada como entrada em uma Rede Neural Artificial de 630 neurônios de entrada, 4.000 neurônios na camada oculta e 6 neurônios na camada de saída, que foram convertidos para o seu código Unicode, utilizado no Microsoft Office. Mil amostras de vários tipos de letras foram utilizadas para o treinamento, obtendo um aproveitamento de 90\% na etapa de processamento das imagens e de 85\% no reconhecimento.
		
		\subsection{Comparativo dos trabalhos relacionados}
		\label{sec:comp_trabalhosrelacionados}
		
		O Quadro \ref{qua:comparativo1} apresenta um resumo comparativo entre os trabalhos relacionados.
		A primeira coluna apresenta o(s) autor(es), seguido do problema proposto, o algoritmo de reconhecimento utilizado, a taxa de acerto e sua importância para este trabalho, respectivamente.
			
		\begin{quadro}[H]
			\centering
			\legenda[qua:comparativo1]{Resumo comparativo dos trabalhos relacionados}
			\begin{tabular}{| c | c | c | c | p{6cm} |}
				\hline \textbf{\makecell{Autor(es)}} & \textbf{\makecell{Problema}} & \textbf{\makecell{Algo-\\ritmo}} & \textbf{\makecell{Taxa Acerto}} & \textbf{\makecell{Importância}}\\ \hline 
				
				\makecell{Garris \\ \textit{et al.} \\ (1997)} & \makecell{Extração de \\dados de\\ formulários} & \makecell{RNA,\\PDI} & \makecell{95\% \\(maiúsculas),\\90\% \\(dígitos),\\85\% \\(minúsculas)} &\vspace{-1.7cm} Os autores descrevem o funcionamento do sistema NIST, o \textit{software} gerador do \textit{dataset} utilizado neste trabalho. Descrevem também a estrutura do código-fonte do mesmo.\\ \hline
				
				\makecell{Cohen \\ \textit{et al.} \\ (2017)} & \makecell{Comparativo \\entre técnicas\\ de IA} & \makecell{RNA,\\PDI} & \makecell{RNA foi \\ entre 10\% \\ e 20\% \\melhor} & \vspace{-1.2cm} Os autores descrevem sobre a estrutura do \textit{dataset} desenvolvido por eles e utilizado neste trabalho.\\ \hline	
				
				\makecell{Gomes\\ \textit{et al.}\\ (2014)} & \makecell{Reconhecimento\\ de sinalizações \\verticais \\de vias de \\trânsito} & \makecell{RNA,\\PDI} & \makecell{45,3\%\\(detecção),\\96,6\%\\ (reconheci-\\mento)} &\vspace{-1.4cm}Os autores descrevem as etapas necessárias para realizar o reconhecimento e sobre a importância do processo de segmentação para o desempenho.\\ \hline 	
				
				\makecell{Osório\\(1991)} & \makecell{Sistema de\\ reconhecimento \\de caracteres \\variados} & \makecell{RNA,\\PDI} & \makecell{Próximo\\de 100\%} &\vspace{-1.2cm}O autor descreve alguns problemas existentes no Reconhecimento Ótico de Caracteres e o funcionamento de sistemas dessa natureza.\\ \hline 
				
				\makecell{Ferreira\\\textit{et al.}\\(2012)} & \makecell{Reconhecimento\\de placas de\\veículos} & \makecell{RNA,\\PDI} & \makecell{Próximo\\de 100\%} &\vspace{-0.9cm}Os autores descrevem algumas técnicas que também são utilizadas em sistemas de reconhecimento de caracteres escritos à mão, como a extração das letras e a correção do ponto de fuga.\\ \hline 
				
				\makecell{Jan\\\textit{et al.}\\(2016)} & \makecell{Sistema de\\reconhecimento\\de caracteres\\em \textit{Saraiki}} & \makecell{RNA,\\PDI} & \makecell{90\%\\ (processa-\\mento), 85\%\\(reconheci-\\mento)} &\vspace{-1.4cm}Os autores descrevem os desafios ao reconhecer caracteres que possuem natureza cursiva (no caso da língua \textit{Saraiki}), que são comuns aos de reconhecer outros tipos de caracteres manuscritos.\\\hline	
			\end{tabular}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
				
		\chapter{Metodologia}
		\label{cap:metodologia}
		
		Este capítulo descreve sobre a base de dados utilizada, sobre a biblioteca utilizada e sobre a linguagem de programação a qual o algoritmo de treinamento e testes foi desenvolvida, sobre os cenários dos testes realizados e sobre o código-fonte do algoritmo desenvolvido.
				
		\section{Base de dados}
		\label{sec:base}
		
		O problema proposto envolve uma análise de desempenho utilizando RNAs no reconhecimento ótico de dígitos escritos à mão. Para isso será utilizada uma base de treinamento e de testes desenvolvida por \citeonline{cohen2017emnist}, o EMNIST, composta por 280.000 amostras de dígitos escritos à mão. Tal base será utilizada pelo fato de ser bastante conhecida e pelo foco do trabalho ser a realização da análise, garantindo, assim, a integridade dos dados a serem utilizados no treinamento da RNA. A escolha do uso de RNAs e do uso da base de treinamento de \citeonline{cohen2017emnist} para esse trabalho se deve as seguintes características:
		
		\begin{lista}
			\item Relevância: A área de estudo sobre RNAs tem crescendo com maior velocidade.
					
			\item Complexidade do algoritmo: Uma RNA possui muitas variáveis de configuração logo, possui uma abrangência muito maior de análise do que outros algoritmos. 
			
			\item Quantidade de dados: Uma RNA necessita de mais dados de treinamento que outros algoritmos mais simples. Como o problema proposto envolve o uso da base de dados desenvolvida por Cohen \textit{et al.} (2017), composta por mais de 200.000 amostras de treinamento, o uso de RNAs é o mais adequado.
			
			\item Qualidade dos dados: O desenvolvimento de todas as etapas de um sistema de reconhecimento ótico de dígitos escritos à mão para realizar a análise exigiria um tempo maior de desenvolvimento para garantir uma precisão maior na etapa de segmentação e alta qualidade dos dados utilizados no treinamento da RNA.
			
		\end{lista}
		
		
		A base de dados é composta por amostras distribuídas de forma igual entre 10 classes, sendo cada amostra composta por 784 atributos de entrada, onde cada uma corresponde a um \textit{pixel} da imagem resultante da etapa de extração de características do sistema NIST, desenvolvido por \citeonline{garris1997nist}. A base de treinamento está armazenada no formato de arquivo .csv, composto 240.000 linhas, onde cada linha é uma amostra, dividida por 785 colunas no qual a primeira coluna representa a saída esperada e as demais, os 784 \textit{pixels}, que são os atributos. A base de teste segue a mesma estrutura da anterior, com exceção do número de linhas o qual é menor~(40.000 amostras).
		
		\section{Biblioteca \textit{scikit-learn} e \textit{Python}}
		
		Para o desenvolvimento e a análise dos dados escolheu-se a linguagem de programação \textit{Python} por ser de natureza interpretada, o que garante uma facilidade de uso maior ao realizar tarefas que envolvem manipulação de dados em massa, além de possuir muitas bibliotecas de diversos tipos. 
		
		Uma das bibliotecas é a \textit{scikit-learn}, desenvolvida por \citeonline{scikit-learn}, a qual possui uma gama de algoritmos voltados para o Aprendizado de Máquina, sendo uma das principais e mais utilizadas bibliotecas do gênero, possuindo boa facilidade de uso. Por esse motivo, essa biblioteca foi utilizada neste trabalho.
			
		\section{Cenários de Testes}
		\label{sec:cenarios}
		Para a realização dos testes, considerou-se a variação da RNA nos seguintes aspectos: (1)~número de camadas ocultas, (2)~número de neurônios em cada camada oculta, (3)~taxa de aprendizado, (4)~\textit{momentum}, (5)~função de ativação, (6)~tamanho do lote, (7)~número de ciclos e (8)~normalização dos dados de treinamento. A análise abrange apenas RNAs multicamadas, uma vez que são as mais adequadas para os dados que foram utilizados como apoio. Todas as RNAs possuem 784 entradas e 10 saídas, uma saída para cada classe. Logo, todas as RNAs possuem 784 neurônios na camada de entrada e 10 neurônios na camada de saída.
		
		Para realizar a análise, foi necessário definir valores padrões que foram fixados nas situações onde aquele parâmetro não estaria sendo avaliado, sendo possível determinar a influência de um único parâmetro, isolando os demais.\hspace{0.1cm}Os valores padrões e os cenários de testes são apresentados nos Quadros de \ref{qua:cen_taxa} até \ref{qua:cen_numciclos}. Vale frisar que o \textit{Alpha} que, embora seja utilizado como padrão pelo \textit{scikit-learn} e existentes nos cenários de testes, não será feita a análise focando o mesmo.
			
		\begin{quadro}[H]
			\centering
			\legenda[qua:cen_taxa]{Cenário de teste da taxa de aprendizado}
			
			\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{Parâmetros}} &
				\textbf{\makecell{Valores}}
				\\ \hline
				
				\textbf{\makecell{Taxa de aprendizado}} & \textbf{\makecell{Variação entre 0,1 e 1,0,\\em saltos de 0,1}}\\ \hline
				
				\makecell{\textit{Momentum}} & \makecell{0,0}\\ \hline
				
				\makecell{Tamanho do lote} & \makecell{234}\\ \hline
				
				\makecell{Número de ciclos} & \makecell{1}\\ \hline
				
				\makecell{Número de camadas ocultas} & \makecell{1}\\ \hline
				
				\textbf{\makecell{Número de neurônios na(s) camada(s) oculta(s)}} & \textbf{\makecell{10, 25, 50, 100, 200, 300, 400,\\500, 1000 e 2000}}\\ \hline
				
				\textbf{\makecell{Funções de ativação}} & \textbf{\makecell{Tangente hiperbólica e logística}}\\ \hline
				
				\textbf{\makecell{Base de treinamento}} & \textbf{\makecell{Normalizada e não normalizada}}\\ \hline
				
				\makecell{\textit{Alpha}} & \makecell{0,0001}\\ \hline
			\end{tabular}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:cen_momen]{Cenário de teste do \textit{momentum}}
			
			\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{Parâmetros}} &
				\textbf{\makecell{Valores}}
				\\ \hline
				
				\makecell{Taxa de aprendizado} & \makecell{0,1}\\ \hline
				
				\textbf{\makecell{\textit{Momentum}}} & \textbf{\makecell{Variação entre 0,0 e 1,0,\\em saltos de 0,1}}\\ \hline
				
				\makecell{Tamanho do lote} & \makecell{234}\\ \hline
				
				\makecell{Número de ciclos} & \makecell{1}\\ \hline
				
				\makecell{Número de camadas ocultas} & \makecell{1}\\ \hline
				
				\textbf{\makecell{Número de neurônios na(s) camada(s) oculta(s)}} & \textbf{\makecell{10, 25, 50, 100, 200, 300, 400,\\500, 1000 e 2000}}\\ \hline
				
				\textbf{\makecell{Funções de ativação}} & \textbf{\makecell{Tangente hiperbólica e logística}}\\ \hline
				
				\textbf{\makecell{Base de treinamento}} & \textbf{\makecell{Normalizada e não normalizada}}\\ \hline
				
				\makecell{\textit{Alpha}} & \makecell{0,0001}\\ \hline
			\end{tabular}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:cen_num_camadas]{Cenário de teste do número de camadas ocultas e número de neurônios nas mesmas}
			
			\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{Parâmetros}} &
				\textbf{\makecell{Valores}}
				\\ \hline
				
				\makecell{Taxa de aprendizado} & \makecell{0,1}\\ \hline
				
				\makecell{\textit{Momentum}} & \makecell{0,0}\\ \hline
				
				\makecell{Tamanho do lote} & \makecell{234}\\ \hline
				
				\makecell{Número de ciclos} & \makecell{1}\\ \hline
				
				\textbf{\makecell{Número de camadas ocultas}} & \textbf{\makecell{Entre 1 e 4, em saltos de 1}}\\ \hline
				
				\textbf{\makecell{Número de neurônios na(s) camada(s) oculta(s)}} &\textbf{ \makecell{10, 50, 100, 500 e 1000}}\\ \hline
				
				\textbf{\makecell{Funções de ativação}} & \textbf{\makecell{Tangente hiperbólica e logística}}\\ \hline
				
				\textbf{\makecell{Base de treinamento}} & \textbf{\makecell{Normalizada e não normalizada}}\\ \hline
				
				\makecell{\textit{Alpha}} & \makecell{0,0001}\\ \hline
			\end{tabular}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
			
		\begin{quadro}[H]
			\centering
			\legenda[qua:cen_batch]{Cenário de teste do tamanho do lote (\textit{batch size})}
			
			\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{Parâmetros}} &
				\textbf{\makecell{Valores}}
				\\ \hline
				
				\makecell{Taxa de aprendizado} & \makecell{0,1}\\ \hline
				
				\makecell{\textit{Momentum}} & \makecell{0,0}\\ \hline
				
				\textbf{\makecell{Tamanho do lote}} & \textbf{\makecell{1, 3, 7, 14, 29, 58, 117, 234,\\468, 937, 1875, 3750 e 7500}}\\ \hline
				
				\makecell{Número de ciclos} & \makecell{1}\\ \hline
				
				\makecell{Número de camadas ocultas} & \makecell{1}\\ \hline
				
				\textbf{\makecell{Número de neurônios na(s) camada(s) oculta(s)}} & \textbf{\makecell{10, 50, 100, 500 e 1000}}\\ \hline
				
				\textbf{\makecell{Funções de ativação}} & \textbf{\makecell{Tangente hiperbólica e logística}}\\ \hline
				
				\textbf{\makecell{Base de treinamento}} & \textbf{\makecell{Normalizada e não normalizada}}\\ \hline
				
				\makecell{\textit{Alpha}} & \makecell{0,0001}\\ \hline
			\end{tabular}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:cen_numciclos]{Cenário de teste do número de ciclos}
			
						\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{Parâmetros}} &
				\textbf{\makecell{Valores}}
				\\ \hline
				
				\makecell{Taxa de aprendizado} & \makecell{0,1}\\ \hline
				
				\makecell{\textit{Momentum}} & \makecell{0,0}\\ \hline
				
				\makecell{Tamanho do lote} & \makecell{234}\\ \hline
				
				\textbf{\makecell{Número de ciclos}} & \textbf{\makecell{1, 10, 20, 30, 40, 50, 100,\\200, 300, 400, 500 e 1000}}\\ \hline
				
				\makecell{Número de camadas ocultas} & \makecell{1}\\ \hline
				
				\textbf{\makecell{Número de neurônios na(s) camada(s) oculta(s)}} & \textbf{\makecell{10, 50, 100, 500 e 1000}}\\ \hline
				
				\textbf{\makecell{Funções de ativação}} & \textbf{\makecell{Tangente hiperbólica e logística}}\\ \hline
				
				\textbf{\makecell{Base de treinamento}} & \textbf{\makecell{Normalizada e não normalizada}}\\ \hline
				
				\makecell{\textit{Alpha}} & \makecell{0,0001}\\ \hline
			\end{tabular}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Para avaliar as funções de ativação e a normalização da base de treinamento, utilizou-se os resultados obtidos em todos os cenários descritos. 
		
		\section{Código-Fonte}
		
		Para realizar a análise foi desenvolvido um algoritmo, cujo funcionamento é apresentado na Figura \ref{fig:func_alg}.
		
		\begin{figure}[H]
			\legenda[fig:func_alg]{Funcionamento do algoritmo de treinamento e teste desenvolvido}
			\fig{scale=0.7}{imagens/algoritmo_treinamento}
			\fonte{Próprio autor, 2018}
		\end{figure}
		
		O funcionamento do algoritmo de treinamento e testes das RNAs consiste em carregar a base de treinamento e de testes desenvolvidas por Cohen \textit{et\hspace{0.1cm}al.} (2017). Além disso, o algoritmo recebe também as configurações de entrada por linha de comando, que indicam quais as configurações utilizadas no treinamento.\hspace{0.1cm}Após isso, o algoritmo treina um conjunto de RNAs, realizando combinações envolvendo os parâmetros de entrada. A saída do algoritmo é composta por um conjunto de arquivos binários, um para cada RNA treinada, e um arquivo texto com os resultados dos testes. Para cada RNA é medida a taxa de acerto mesma, para a base de testes. A base de treinamento é composta por 240.000 amostras e a de testes por 40.000 amostras.
		
		O algoritmo desenvolvido é dividido em cinco módulos:
		
		\begin{lista}
			
			\item Bibliotecas.py: Arquivo responsável por importar as bibliotecas utilizadas no algoritmo.\hspace{0.1cm}Além da \textit{scikit-learn}, o algoritmo utiliza algumas bibliotecas de manipulação de arquivos e de matrizes, como a biblioteca \textit{Numpy}, \textit{Scipy} e outras pertencentes ao próprio \textit{Python}.	
			
			\item MainTreinamento.py: Início do algoritmo de treinamento e teste.
			
			\item EntradaUsuario.py: Responsável por realizar o tratamento dos parâmetros de configuração inseridos pelo usuário.\hspace{0.1cm}Os parâmetros de configuração são citados na seção \ref{sec:cenarios}.
			
			\item ManipArquivosDataset.py: Responsável por realizar o carregamento das bases de treinamento e de testes e normalizar as mesmas.

			\item TreinamentoRedeNeural.py: Responsável por realizar o treinamento e teste das Redes Neurais Artificiais a partir dos dados de entrada já tratados.
		\end{lista}
		
		Já as funções implementadas dentro de cada módulo são:
		
		\begin{lista}
			\item MainTreinamento.py:
			\begin{lista}
				\item[•] \texttt{Start()}:\hspace{0.1cm}Função executada a partir do arquivo MainTreinamento.py. Inicia o treinamento recebendo os dados via linha de comando.
				
				\item[•] \texttt{MainP2()}:\hspace{0.1cm}Função executada caso seja necessário o uso de \textit{threads}.\hspace{0.1cm}Essa função recebe como parâmetros os dados de entrada por meio de uma \textit{string} e não por linha de comando, como no item anterior.\hspace{0.1cm}Também recebe a base de treinamento e de testes já carregadas na memória.
			\end{lista}
		
			\item EntradaUsuario.py:
			\begin{lista}
				\item[•] \texttt{Start2()}:\hspace{0.1cm}Função executada após ser chamada a função \texttt{MainP2()}, tal função recebe os parâmetros e inicia o treinamento.
				
				\item[•] \texttt{TratamentoLinhaComando()}:\hspace{0.1cm}Serve para realizar o tratamento da entrada do usuário, seja por \textit{string} ou por linha de comando. Faz com que seja exibida uma tela de ajuda ao executar o arquivo \texttt{MainTreinamento.py} com o parâmetro \texttt{-h}.
				
				\item[•] \texttt{Gerador()}:\hspace{0.1cm}Serve para gerar uma \textit{string} de entrada, a qual contém combinações de topologias de camadas ocultas. Também utilizada para a entrada por \textit{string} recebendo uma lista com as combinações de neurônios e o tamanho para cada combinação.
			\end{lista}
		
			\item ManipArquivosDataset.py:
			\begin{lista}
				\item[•] \texttt{CarregarDataset()}:\hspace{0.1cm}Função que carrega os \textit{datasets} de treinamento e de testes e armazena-os em memória. Recebe o diretório do arquivo.
				
				\item[•] \texttt{GravarResultadosTeste()}:\hspace{0.1cm}Escreve no arquivo texto o resultado da RNA recém treinada.
				
				\item[•] \texttt{GravarRedeNeuralTreinada()}:\hspace{0.1cm}Cria um arquivo binário e salva os dados da RNA treinada nesse arquivo.  
				
				\item[•] \texttt{NormalizarDataset()}:\hspace{0.1cm}Realiza o processo de normalização dos dados, recebendo a base de treinamento e de testes.
				
				\item[•] \texttt{FormatarSaidaTXT()}:\hspace{0.1cm}Formata os dados a serem escritos no arquivo texto com os resultados.
				
				\item[•] \texttt{FormatarNomeArquivo()}:\hspace{0.1cm}Gera um nome único para o arquivo binário, utilizando os dados da iteração atual.
			\end{lista}
			
			\item TreinamentoRedeNeural.py:
			\begin{lista}
				\item[•] \texttt{IniciaTreinoTeste()}:\hspace{0.1cm}Função que inicializa o processo de treinamento e teste da RNA. Recebe os dados lidos da entrada do usuário e o \textit{dataset} de treinamento e de testes.
				
				\item[•] \texttt{TreinoTeste()}: Função que faz o treinamento e o teste da RNA, combinando os parâmetros de entrada. Também realiza a gravação dos dois arquivos, binário com a RNA treinada e uma linha no arquivo texto com os resultados.
			\end{lista}
		\end{lista}
		
		Durante o desenvolvimento do trabalho, devido a grande quantidade de combinações a serem feitas, foi necessária a utilização de paralelismo~(\textit{threads}) para treinar mais de uma RNA ao mesmo tempo, diminuindo o tempo de treinamento de todas as combinações. Para isso, foi necessário desenvolver arquivos adicionais. Os arquivos que implementam o paralelismo, utilizando como apoio os módulos descritos, são:
		
		\begin{lista}
			\item \texttt{MainParalelismoBatchSizeNumCiclos.py}: Realiza o treinamento e os testes das combinações para os cenários do número de ciclos e do tamanho do lote.
			
			\item \texttt{MainParalelismoTaxaAprendizado.py}: Realiza o treinamento e os testes das combinações para os cenários da taxa de aprendizado e do \textit{momentum}.
			
			\item \texttt{MainParalelismoNumCamadasOcultas.py}: Realiza o treinamento e os testes das combinações para os cenários do número de camadas ocultas e do número de neurônios em cada camada oculta.
		\end{lista}
		
		Para ambos os arquivos-fontes, foi desenvolvido uma função chamada de \texttt{MainP1()} que realiza o carregamento das bases de treinamento e de testes, chamando a função \texttt{CarregarDataset()}. Após, é feita a normalização das bases de treinamento e de testes, chamando a função \texttt{NormalizarDataset()}.\hspace{0.1cm}Depois, é criada 4 threads, passando como parâmetros a função \texttt{MainP2()}, as bases de treinamento e de testes. Também é passada por parâmetro a \textit{string} de entrada que indica como será feito o treinamento, tratada pela função \texttt{TratamentoLinhaComando()}.\hspace{0.1cm}A partir de então, o algoritmo realiza o treinamento de todas as combinações de RNAs, utilizando os parâmetros passados pela \textit{string}, e em paralelo pois é possível passar para cada \textit{thread}, uma parte das combinações necessárias para cada cenário de teste. 
		
		\chapter{Resultados Experimentais}
		\label{cap:resultados}
		
		\section{Critérios de avaliação}
		
		Para avaliar o desempenho conforme a variação de cada um dos parâmetros, os seguintes indicadores foram utilizados: 
		
		\begin{lista}
					
			\item[1.] Desempenho médio (\%): Composto pela média aritmética de todos os resultados obtidos para o mesmo quadro, resultante da Equação \ref{eq:calc_med_arit}.
			
			\begin{equation}
				\label{eq:calc_med_arit}
				Desempenho_{Medio} = \frac{\sum_{i = 1}^{n}{x_i}}{n}
			\end{equation}			
			
			Onde $x_i$ é o elemento de número $i$ presente no grupo de resultados obtidos e $n$ é o número de elementos desse grupo.
			
			\item[2.] Variação média (\%): Composto pelo resultado da média aritmética de todas as variações de desempenho para o mesmo quadro. Tem como objetivo determinar se houve melhora ou piora no desempenho conforme a variação do parâmetro testado e é resultado da Equação \ref{eq:calc_var_med}.
			
			\begin{equation}
				\label{eq:calc_var_med}
				Variacao_{Media} = \frac{\sum_{i = 1}^{n}{(x_i - x_{(i+1)}})}{(n-1)}
			\end{equation}
			
			Onde $x_i$ e $x_{(i+1)}$ são os elementos de números $i$ (atual) e $i+1$ (próximo) presentes no grupo de resultados obtidos e $n-1$ é o número de elementos desse grupo, excluindo o primeiro elemento por não possuir um anterior.
						
			\item[3.] Desvio padrão: Composto pelo resultado do cálculo do desvio padrão para todos os resultados pertencentes ao mesmo quadro. Tem como objetivo determinar o grau de oscilação do desempenho, considerando $Desempenho_{Medio}$ como referência conforme a variação do parâmetro testado. Seu cálculo é descrito pela Equação \ref{eq:calc_desv_pad}.		
			
			\begin{equation}
				\label{eq:calc_desv_pad}
				Desvio_{Padrao} = \sqrt{\frac{\sum_{i = 1}^{n}{(x_i - Desempenho_{Medio})}}{(n)}}
			\end{equation}
			
			Onde $x_i$ é o elemento de número $i$ presente no grupo de resultados obtidos e $n$ é o número de elementos desse grupo. $Desempenho_{Medio}$ é o resultado obtido da Equação \ref{eq:calc_med_arit}.
			
			\item[4.] Desempenho médio global (\%): Resultante da média entre todos os desempenhos médios de cada quadro analisado, resultantes da Equação \ref{eq:calc_med_arit}.
			
			\item[5.] Variação média global (\%): Resultante da média entre todas as variações médias de cada quadro analisado (Equação \ref{eq:calc_var_med}).
			
			\item[6.] Desvio padrão global (\%): Resultante da média entre todos os desvios padrões de cada quadro analisado (Equação \ref{eq:calc_desv_pad}). 
			
			\item[7.] Melhor desempenho médio (\%): Melhor desempenho médio considerando todos os quadros testados, sendo que cada quadro possui um desempenho médio.
			
			\item[8.] Pior desempenho médio (\%): Pior desempenho médio considerando todos os quadros testados. sendo que cada quadro possui um desempenho médio.
		\end{lista}
		
		\section{Resultados}
		
		Os resultados obtidos para cada um dos parâmetros avaliados são descritos a seguir, sendo que os dados obtidos nos testes podem ser vistos no \textit{pendrive} anexo a esse trabalho.
		
		\subsection{Taxa de aprendizado}
		
		Os resultados para cada uma das funções de ativação e base de treinamento para a análise da taxa de aprendizado são apresentados no Quadro \ref{qua:result_taxa}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_taxa]{Resultados da análise para a taxa de aprendizado}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 34,9244 & 41,6305 & \textbf{25,565} & -6,9199 & 21,9649 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{95,077} & \textbf{96,8357} & 91,9462 & 0,0754 & \textbf{0,3762} \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 69,7519 & 77,6015 & 54,9105 & -5,8235 & 18,7213 \\ \hline
				
				\makecell{Logistic \\ (Normalizada)} & 94,7171 & 95,8545 & 92,4547 & \textbf{0,2611} & 0,7973 \\ \hline	
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar a variação média global existente no Quadro \ref{qua:result_taxa}, percebe-se que no intervalo de crescimento da taxa de aprendizado de $0,1$ até $1$, houve uma queda no desempenho das RNAs não normalizadas~(-6,9199 para a função tangente hiperbólica, -5,8235 para a função logística).\hspace{0.1cm}Para as RNAs normalizadas, houve um pequeno crescimento~(0,0754 para a função tangente hiperbólica, 0,2611 para a função logística).\hspace{0.1cm}Os melhores resultados para a taxa de aprendizado, considerando o número de neurônios, função de ativação e normalização, são apresentados no Quadro \ref{qua:result_varia_taxa}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_varia_taxa]{Melhores valores para a taxa de aprendizado}
			{\footnotesize
				\begin{tabular}{| c | c | c | c | c | c |}
					\hline \textbf{\makecell{Número de\\Neurônios}} & \textbf{\makecell{Tanh\\(Não Normalizada)}} &
					\textbf{\makecell{Tanh\\(Normalizada)}} & \textbf{\makecell{Logistic\\(Não Normalizada)}} & \textbf{\makecell{Logistic\\(Normalizada)}} \\ \hline
														
					\makecell{10} & 0,1 & 0,2 & 0,1 & 0,6 \\ \hline
					\makecell{25} & 0,1 & 0,3 & 0,1 & 0,7 \\ \hline
					\makecell{50} & 0,1 & 0,6 & 0,1 & 1,0 \\ \hline
					\makecell{100} & 0,1 & 0,7 & 0,1 & 1,0 \\ \hline
					\makecell{200} & 0,1 & 1,0 & 0,1 & 0,8 \\ \hline
					\makecell{300} & 0,1 & 0,8 & 0,1 & 1,0 \\ \hline
					\makecell{400} & 0,1 & 0,7 & 0,1 & 1,0 \\ \hline
					\makecell{500} & 0,1 & 0,7 & 0,1 & 1,0 \\ \hline
					\makecell{1000} & 0,1 & 0,9 & 0,1 & 1,0 \\ \hline
					\makecell{2000} & 0,1 & 0,6 & 0,1 & 0,9 \\ \hline
				\end{tabular}
			}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar o Quadro \ref{qua:result_varia_taxa} percebe-se que para as RNAs não normalizadas a maioria dos melhores resultados foram obtidos utilizando o valor $0,1$ para a taxa de aprendizado.\hspace{0.1cm}Já para as RNAs normalizadas, com a função tangente hiperbólica, a maioria dos melhores resultados foram obtidos utilizando os valores entre $0,6$ e $1$ para a taxa de aprendizado. Para a função logística, a maioria dos melhores resultados foram obtidos utilizando os valores entre $0,8$ e $1$ para a taxa de aprendizado.
		
		\subsection{\textit{Momentum}}
		
		Os resultados para cada uma das funções de ativação e base de treinamento para a análise do \textit{momentum} são apresentados no Quadro \ref{qua:result_momentum}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_momentum]{Resultados da análise para o \textit{momentum}}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 58,1814 & 69,9293 & \textbf{32,7945} & -5,4476 & 21,4834 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{95,1823} & \textbf{96,0986} & 91,1025 & -0,6962 & 2,4089 \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 81,6893 & 88,1427 & 64,4425 & -6,2987 & 19,3166 \\ \hline
				
				\makecell{Logistic \\ (Normalizada)} & 93,7873 & 94,5493 & 90,7675 & \textbf{-0,3449} & \textbf{1,822} \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar a variação média global, existente no Quadro \ref{qua:result_momentum}, percebe-se que no intervalo de crescimento do \textit{momentum} de $0$ até $1$ houve uma queda no desempenho das RNAs não normalizadas~(-5,4476 para a função tangente hiperbólica, -6,2987 para a função logística) e normalizadas~(-0,6962 para a função tangente, -0,3449 para a função logística).\hspace{0.1cm}Os melhores resultados para o \textit{momentum}, considerando o número de neurônios, função de ativação e normalização, são apresentados no Quadro \ref{qua:result_varia_momentum}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_varia_momentum]{Melhores valores para o \textit{momentum}}
			{\footnotesize
				\begin{tabular}{| c | c | c | c | c | c |}
					\hline \textbf{\makecell{Número de\\Neurônios}} & \textbf{\makecell{Tanh\\(Não Normalizada)}} &
					\textbf{\makecell{Tanh\\(Normalizada)}} & \textbf{\makecell{Logistic\\(Não Normalizada)}} & \textbf{\makecell{Logistic\\(Normalizada)}} \\ \hline
					
					\makecell{10} & 0,2 & 0,3 & 0,0 & 0,6 \\ \hline
					\makecell{25} & 0,0 & 0,8 & 0,0 & 0,7 \\ \hline
					\makecell{50} & 0,1 & 0,8 & 0,0 & 1,0 \\ \hline
					\makecell{100} & 0,0 & 0,8 & 0,0 & 1,0 \\ \hline
					\makecell{200} & 0,0 & 0,8 & 0,0 & 0,8 \\ \hline
					\makecell{300} & 0,1 & 0,9 & 0,1 & 1,0 \\ \hline
					\makecell{400} & 0,0 & 0,9 & 0,0 & 1,0 \\ \hline
					\makecell{500} & 0,0 & 0,8 & 0,0 & 1,0 \\ \hline
					\makecell{1000} & 0,1 & 0,9 & 0,0 & 1,0 \\ \hline
					\makecell{2000} & 0,1 & 0,8 & 0,0 & 0,9 \\ \hline
				\end{tabular}
			}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar o Quadro \ref{qua:result_varia_momentum} percebe-se que para as RNAs não normalizadas, a maioria dos melhores resultados foram obtidos utilizando valores entre $0$ e $0,1$ para o \textit{momentum}.\hspace{0.1cm}Já para as RNAs normalizadas, a maioria dos melhores resultados foram obtidos utilizando os valores $0,8$ e $0,9$ para o \textit{momentum}.
		
		\subsection{Número de camadas ocultas}
		
		Os resultados para cada uma das funções de ativação e base de treinamento para a análise do número de camadas ocultas são apresentados nos Quadros \ref{qua:result_numcamadas1} à \ref{qua:result_numcamadas4b}.
		
		\subsubsection{1ª Camada oculta}
		
		Para analisar a variação no desempenho conforme a adição de camadas ocultas na RNA, foi utilizado o Quadro \ref{qua:result_numcamadas1} como referência.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas1]{Resultados da análise para a 1ª camada oculta}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 76,464 & 90,355 & \textbf{49,603} & \textbf{8,1238} & 10,241 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{94,814} & 95,67 & 92,093 & 0,8744 & 1,058 \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 91,433 & \textbf{96,94} & 81,705 & 3,6163 & 4,1306 \\ \hline
				
				\makecell{Logistic \\ (Normalizada)} & 93,0565 & 93,793 & 90,94 & 0,6394 & \textbf{0,8166} \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		\subsubsection{2ª Camada oculta}
		
		Para analisar os resultados do Quadro \ref{qua:result_numcamadas2}, deve-se antes realizar uma comparação entre os resultados dos Quadros \ref{qua:result_numcamadas1} e \ref{qua:result_numcamadas2}. Os resultados podem ser vistos no Quadro \ref{qua:result_numcamadas2b}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas2]{Resultados da análise para a 2ª camada oculta}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 74,355 & 89,442 & \textbf{54,19} & \textbf{2,5412} & 4,4683 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{95,1712} & \textbf{96,2355} & 93,023 & 0,1762 & \textbf{0,2407} \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 86,844 & 95,668 & 69,287 & 0,0416 & 1,97 \\ \hline
				
				\makecell{Logistic \\ (Normalizada)} & 88,423 & 91,635 & 73,4165 & 0,0392 & 2,3582 \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}	
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas2b]{Diferença entre os resultados dos Quadros \ref{qua:result_numcamadas1} e \ref{qua:result_numcamadas2}}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Diferença\\do\\Desempenho\\Médio\\Global}} & 
				\textbf{\makecell{Diferença\\do\\Melhor\\Desempenho}} & 
				\textbf{\makecell{Diferença\\do\\Pior\\Desempenho}} &
				\textbf{\makecell{Diferença\\da\\Variação\\Média\\Global}} &
				\textbf{\makecell{Diferença\\do\\Desvio\\ Padrão\\Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & -2,109 & -0,913 & 4,587 & -5,5826 & \textbf{-5,7727} \\ \hline
			
				\makecell{Tanh \\ (Normalizada)} & \textbf{0,3572} & \textbf{0,5655} & 0,93 & -0,6982 & -0,8173 \\ \hline
			
				\makecell{Logistic \\ (Não Normalizada)} & -4,589 & -0,502 & -12,418 & -3,5747 & -2,1606 \\ \hline
			
				\makecell{Logistic \\ (Normalizada)} & -4,6335 & -2,158 & \textbf{-17,5235} & \textbf{-0,6002} & 1,5416 \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar a diferença do desempenho médio global, existente no Quadro \ref{qua:result_numcamadas2b}, percebe-se que a adição da 2ª camada oculta ocasionou uma queda no desempenho das RNAs não normalizadas~(-2,109 para a função tangente hiperbólica, -4,589 para a função logística) e normalizadas com a função logística~(-4,6335).\hspace{0.1cm}Para as RNAs normalizadas com função tangente houve um pequeno crescimento~(0,3572).
		
		\subsubsection{3ª Camada oculta}
		
		Para analisar os resultados do Quadro \ref{qua:result_numcamadas3}, deve-se antes realizar a comparação entre os resultados dos Quadros \ref{qua:result_numcamadas2} e \ref{qua:result_numcamadas3}. Os resultados podem ser vistos no Quadro \ref{qua:result_numcamadas3b}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas3]{Resultados da análise para a 3ª camada oculta}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 76,1861 & 91,996 & 47,3565 & 0,4280 & 3,5944 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{95,5238} & \textbf{96,7605} & 92,3745 & 0,1610 & \textbf{0,2147} \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 67,6497 & 93,6695 & 23,82 & 0,1069 & 4,5998 \\ \hline
				
				\makecell{Logistic \\ (Normalizada)} & 32,2082 & 71,0455 & \textbf{10} & \textbf{-2,3897} & 8,0089 \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
				
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas3b]{Diferença entre os resultados dos Quadros \ref{qua:result_numcamadas2} e \ref{qua:result_numcamadas3}}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Diferença\\do\\Desempenho\\Médio\\Global}} & 
				\textbf{\makecell{Diferença\\do\\Melhor\\Desempenho}} & 
				\textbf{\makecell{Diferença\\do\\Pior\\Desempenho}} &
				\textbf{\makecell{Diferença\\da\\Variação\\Média\\Global}} &
				\textbf{\makecell{Diferença\\do\\Desvio\\ Padrão\\Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & \textbf{1,8311} & \textbf{2,554} & -6,8335 & -2,1132 & \textbf{-0,8739} \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & 0,3526 & 0,525 & -0,6485 & -0,0151 & -0,026 \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & -19,1943 & -1,9985 & -45,467 & \textbf{0,0653} & 2,6298 \\ \hline
				
				\makecell{Logistic \\ (Normalizada)} & -56,2148 & -20,5895 & \textbf{-63,4165} & -2,4289 & 5,6507 \\ \hline	
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
			
		Ao analisar a diferença do desempenho médio global, existente no Quadro \ref{qua:result_numcamadas3b}, percebe-se que a adição da 3ª camada oculta ocasionou um aumento no desempenho das RNAs com função tangente hiperbólica~(-1,8311 para a não normalizada, 0,3526 para a normalizada). \hspace{0.1cm}Para as RNAs com função logística houve uma grande queda~(-19,1943 para a não normalizada, -56,2148 para a normalizada).
		
		\subsubsection{4ª Camada oculta}
		
		Para analisar os resultados do Quadro \ref{qua:result_numcamadas4}, deve-se realizar a comparação entre os resultados dos Quadros \ref{qua:result_numcamadas3} e \ref{qua:result_numcamadas4}. Os resultados podem ser vistos no Quadro \ref{qua:result_numcamadas4b}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas4]{Resultados da análise para a 4ª camada oculta}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 74,738 & 96,339 & 46,357 & \textbf{0,6023} & 3,8325 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{94,919}  & \textbf{97,061} & 92,279 & 0,1459 & \textbf{0,2056} \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 14,387 & 50,722 & 10 & -1,0639 & 3,117 \\ \hline
				
				\makecell{Logistic (Normalizada)} & 10,199 & 28,2 & \textbf{9,99995} & -0,0335 & 0,3175 \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_numcamadas4b]{Diferença entre os resultados dos Quadros \ref{qua:result_numcamadas3} e \ref{qua:result_numcamadas4}}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Diferença\\do\\Desempenho\\Médio\\Global}} & 
				\textbf{\makecell{Diferença\\do\\Melhor\\Desempenho}} & 
				\textbf{\makecell{Diferença\\do\\Pior\\Desempenho}} &
				\textbf{\makecell{Diferença\\da\\Variação\\Média\\Global}} &
				\textbf{\makecell{Diferença\\do\\Desvio\\ Padrão\\Global}} \\ \hline
				
				Tanh (Não Normalizada) & -1,4481 & \textbf{4,343} & -0,9995 & 0,1743 & 0,2381 \\ \hline
				
				Tanh (Normalizada) & \textbf{-0,6048} & 0,3005 & -0,0955 & -0,0151 & -0,0091 \\ \hline
				
				Logistic (Não Normalizada) & -53,2627 & -42,9475 & -13,82 & -1,1708 & -1,4828 \\ \hline
				
				Logistic (Normalizada) & -22.0092 & -42,8455 & \textbf{-0,00005} & \textbf{2,3562} & \textbf{-7,6914} \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar a diferença do desempenho médio global, existente no Quadro \ref{qua:result_numcamadas4b}, percebe-se que a adição da 4ª camada oculta ocasionou uma queda no desempenho de todas as RNAs com função tangente hiperbólica~(-1,4481 para a não normalizada, -0,6048 para a normalizada) e função logística~(-53,2627 para a não normalizada, -22,0092 para a normalizada).
		
		\subsection{Número de neurônios na camada oculta}
		
		Ao analisar a variação média global, existentes nos Quadros \ref{qua:result_numcamadas1}, \ref{qua:result_numcamadas2}, \ref{qua:result_numcamadas3} e \ref{qua:result_numcamadas4}, percebe-se que, para o número de neurônios desse cenário de teste, existentes no Quadro \ref{qua:cen_num_camadas}, houve um aumento no desempenho de todas as RNAs com função tangente hiperbólica~(8,1238, 2,5412, 0,4280 e 0,6023 para a não normalizada; 0,8744, 0,1762, 0,1610 e 0,1459 para a normalizada, ambas para a 1ª, 2ª, 3ª e 4ª camada oculta, respectivamente). Para as RNAs com função logística, também houve um crescimento, porém somente até a 3ª camada oculta para a não normalizada~(3,6163, 0,0416, 0,1069 e -1,0639, para a 1ª, 2ª, 3ª e 4ª camada oculta, respectivamente) e até a 2ª camada oculta para a normalizada~(0,6394, 0,0392, -2,3897 e -0,0335, para a 1ª, 2ª, 3ª e 4ª camada oculta, respectivamente).
			
		Já ao analisar a diferença na variação média global, existentes nos Quadros \ref{qua:result_numcamadas2b}, \ref{qua:result_numcamadas3b} e \ref{qua:result_numcamadas4b}, percebe-se que a adição de neurônios nas primeiras camadas ocultas possuem maior impacto no desempenho do que nas camadas subsequentes das RNAs testadas~(-5,5826, -2,1132 e 0,1743 para a função tangente hiperbólica não normalizada; -0,6982, -0,0151 e -0,0151 para a função tangente hiperbólica normalizada; -3,5747, 0,0653 e -1,1708 para a função logística não normalizada, e -0,6002, -2,4289 e 2,3562 para a função logística normalizada).
		
		\subsection{Tamanho do lote (\textit{batch size})}
		
		Os resultados para cada uma das funções de ativação e base de treinamento para a análise do tamanho do lote (\textit{batch size}) é visto no Quadro \ref{qua:result_tam_lote}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_tam_lote]{Resultados da análise para o tamanho do lote (batch size)}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 54.5868 & 61,0575 & 40,0062 & \textbf{6,2548} & 29,4064 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{92,8108} & \textbf{94,0375} & \textbf{89,3862} & 0,0873 & \textbf{2,5775} \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 66,9623 & 73,82 & 52,5462 & 5,9191 & 26,7526 \\ \hline
				
				\makecell{Logistic (Normalizada)} & 90,0664 & 92,2223 & 84,3762 & -1,7861 & 5,7158 \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar a variação média global, existente no Quadro \ref{qua:result_tam_lote}, percebe-se que para os tamanhos de lotes analisados~(ver Quadro \ref{qua:cen_batch}), houve um aumento no desempenho das RNAs não normalizadas~(6,2548 para a função tangente hiperbólica, 5,9191 para a função logística).\hspace{0.1cm}Para as normalizadas houve um pequeno aumento com o uso da função da tangente hiperbólica~(0,0873), enquanto para a função logística houve uma queda no desempenho~(-1,7861). Os melhores resultados para o tamanho do lote, considerando cada número de neurônios, função de ativação e normalização, são apresentados no Quadro \ref{qua:result_varia_tamlote}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_varia_tamlote]{Melhores valores para o tamanho do lote}
			{\footnotesize
				\begin{tabular}{| c | c | c | c | c | c |}
					\hline \textbf{\makecell{Número de\\Neurônios}} & \textbf{\makecell{Tanh\\(Não Normalizada)}} &
					\textbf{\makecell{Tanh\\(Normalizada)}} & \textbf{\makecell{Logistic\\(Não Normalizada)}} & \textbf{\makecell{Logistic\\(Normalizada)}} \\ \hline	
					\makecell{10} & 7500 & 117 & 468 & 29 \\ \hline
					\makecell{50} & 1875 & 29 & 468 & 14 \\ \hline
					\makecell{100} & 1875 & 29 & 468 & 14 \\ \hline
					\makecell{500} & 468 & 29 & 468 & 3 \\ \hline
					\makecell{1000} & 1875 & 29 & 468 & 14 \\ \hline
				\end{tabular}
			}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar o Quadro \ref{qua:result_varia_tamlote} percebe-se que para as RNAs não normalizadas, a maioria dos melhores resultados foram obtidos utilizando o valor $1875$ para a função tangente hiperbólica e $468$ para a função logística; ambos para o tamanho do lote.\hspace{0.1cm}Já para as RNAs normalizadas, com a função tangente hiperbólica, a maioria dos melhores resultados foram obtidos utilizando o valor $29$ para a função tangente hiperbólica e o valor $14$ para a função logística; ambos para o tamanho do lote.
		
		\subsection{Número de ciclos}
		
		Os resultados para cada uma das funções de ativação e base de treinamento para a análise do número de ciclos é visto no Quadro \ref{qua:result_num_ciclos}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_num_ciclos]{Resultados da análise para o número de ciclos}
			{\footnotesize
			\begin{tabular}{| c | c | c | c | c | c |}
				\hline \textbf{\makecell{Função de\\ Ativação\\(Base de Treinamento)}} &
				\textbf{\makecell{Desempenho\\Médio\\ Global}} & 
				\textbf{\makecell{Melhor\\Desempenho\\ Médio}} & 
				\textbf{\makecell{Pior\\Desempenho\\ Médio}} &
				\textbf{\makecell{Variação\\Média\\ Global}} &
				\textbf{\makecell{Desvio \\Padrão\\ Global}} \\ \hline
				
				\makecell{Tanh \\ (Não Normalizada)} & 86,4307 & 93,3246 & 70,4965 & \textbf{0,9626} & 3,3186 \\ \hline
				
				\makecell{Tanh \\ (Normalizada)} & \textbf{96,688}  & \textbf{98,5154} & \textbf{94,1742} & 0,1992 & \textbf{0,3922} \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 94,0033 & 97,5463 & 86,124 & 0,2675 & 0,6761 \\ \hline
				
				\makecell{Logistic (Normalizada)} & 96,6087 & 98,069 & 94,4563 & 0,3777 & 0,7594 \\ \hline
			\end{tabular}
		}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar a variação média global, existente no Quadro \ref{qua:result_num_ciclos}, percebe-se que para o número de ciclos desse cenário de teste, existente no Quadro \ref{qua:cen_numciclos}, ocasionou uma melhora no desempenho médio global de todas as RNAs testadas~(0,9626 para a função tangente hiperbólica não normalizada, 0,1992 para a função tangente hiperbólica normalizada, 0,2675 para a função logística não normalizada e 0,3777 para a função logística normalizada).\hspace{0.1cm}Os melhores resultados para o número de ciclos, considerando o número de neurônios, função de ativação e normalização, são apresentados no Quadro \ref{qua:result_varia_numciclos}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:result_varia_numciclos]{Melhores valores para o número de ciclos}
			{\footnotesize
				\begin{tabular}{| c | c | c | c | c | c |}
					\hline \textbf{\makecell{Número de\\Neurônios}} & \textbf{\makecell{Tanh\\(Não Normalizada)}} &
					\textbf{\makecell{Tanh\\(Normalizada)}} & \textbf{\makecell{Logistic\\(Não Normalizada)}} & \textbf{\makecell{Logistic\\(Normalizada)}} \\ \hline
					
					\makecell{10} & 400 & 1000 & 500 & 400 \\ \hline
					\makecell{50} & 500 & 1 & 50 & 400 \\ \hline
					\makecell{100} & 400 & 30 & 50 & 500 \\ \hline
					\makecell{500} & 300 & 1000 & 500 & 200 \\ \hline
					\makecell{1000} & 400 & 500 & 500 & 500 \\ \hline
				\end{tabular}
			}
			\vspace{0.1cm}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		Ao analisar o Quadro \ref{qua:result_varia_numciclos} percebe-se que para as RNAs não normalizadas, a maioria dos melhores resultados foram obtidos utilizando os valores $400$ e $500$ para a função tangente hiperbólica e $50$ e $500$ para a função logística; ambos para o número de ciclos.\hspace{0.1cm}Já para as RNAs normalizadas, com a função tangente hiperbólica, a maioria dos melhores resultados foram obtidos utilizando os valores $500$ e $1000$ para a função tangente hiperbólica e os valores $400$ e $500$ para a função logística; ambos para o número de ciclos.
		
		\subsection{Normalização da base de treinamento}
		
		Analisando o desempenho médio global dos Quadros \ref{qua:result_taxa} à \ref{qua:result_num_ciclos} percebe-se que o uso da normalização fez com que o desempenho das RNAs subissem na maioria dos casos (14 casos de 16 possíveis). Os dois casos onde isso não ocorreu, foram com o número de camadas ocultas maiores que 2 e utilizando a função de ativação logística.
		
		\subsection{Funções de ativação}
		
		Analisando o desempenho médio global dos Quadros \ref{qua:result_taxa} à \ref{qua:result_num_ciclos} percebe-se que o uso da função de ativação logística, com bases de treinamento não normalizadas, ocasionou um melhor desempenho. Já para as bases de treinamento normalizadas a função de ativação da tangente hiperbólica se mostrou melhor.
		
		\begin{comment}
		\subsection{Comparativo entre os parâmetros de configuração}
		
		Para realizar o comparativo entre os parâmetros, visando encontrar o seu grau de influência, devemos realizar alguns cálculos utilizando os quadros anteriores.
		
		\begin{lista}
			
			\item Taxa de aprendizado: Para realizar o comparativo, utilizou-se a variação média para cada função de ativação, existentes no Quadro \ref{qua:result_taxa}.
			
			\item \textit{Momentum}: Para realizar o comparativo, utilizou-se a variação média para cada função de ativação, existentes no Quadro \ref{qua:result_momentumb}.
			
			\item Número de neurônios: Para realizar o comparativo, utilizou-se a média entre as variações médias para cada função de ativação, existentes nos Quadros \ref{qua:result_numcamadas1}, \ref{qua:result_numcamadas2}, \ref{qua:result_numcamadas3} e \ref{qua:result_numcamadas4}.
			
 			\item Número de camadas ocultas: Para realizar o comparativo, utilizou-se a média entre as diferenças no desempenho médio para cada função de ativação, existentes nos Quadros \ref{qua:result_numcamadas2b}, \ref{qua:result_numcamadas3b} e \ref{qua:result_numcamadas4b}.
 			
 			\item Tamanho do lote (\textit{batch size}): Para realizar o comparativo, utilizou-se a variação média para cada função de ativação, existentes no Quadro \ref{qua:result_tam_lote}.
 			
 			\item Número de ciclos: Para realizar o comparativo, utilizou-se a variação média para cada função de ativação, existentes no Quadro \ref{qua:result_num_ciclos}.
 			
 			\item Função de ativação: Para realizar o comparativo, utilizou-se a diferença entre os desempenhos médios das duas funções de ativação testadas, sem normalização, nos Quadros de \ref{qua:result_taxa} até \ref{qua:result_num_ciclos}. Esse valor é único.
 			
 			\item Normalização: Para realizar o comparativo, utilizou-se a diferença no desempenho médio entre as RNAs normalizadas e não normalizadas, independentemente da função de ativação, nos Quadros de \ref{qua:result_taxa} até \ref{qua:result_num_ciclos}. Esse valor é único.
		\end{lista}
		
		Os resultados desse comparativo são vistos no Quadros \ref{qua:comparativo_para} e \ref{qua:comparativo_para2}.
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:comparativo_para]{Comparativo entre os parâmetros de configuração (valores únicos)}
			
			\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{FA}} &
				\textbf{\makecell{N}}
				\\ \hline
				-6,9199 & 1,4723 \\ \hline
			\end{tabular}
		\vspace{0.1cm}
		\fonte{Próprio autor, 2018}
		\end{quadro}
		
		\begin{quadro}[H]
			\centering
			\legenda[qua:comparativo_para2]{Comparativo entre os parâmetros de configuração}
			
			\begin{tabular}{| c | c | c | c | c | c | c | c | c |}
				\hline
				\textbf{\makecell{FA}} &
				\textbf{\makecell{TA}} &
				\textbf{\makecell{M}} &
				\textbf{\makecell{NCO}} & 
				\textbf{\makecell{NN}} & 
				\textbf{\makecell{TL}} &
				\textbf{\makecell{NC}}
				\\ \hline
				\makecell{Tanh \\ (Não Normalizada)} & -6,9199 & 1,4723 & -0,5753 & 2,9238 & 6,2548 & 0,9626 \\ \hline

				\makecell{Tanh \\ (Normalizada)} & 0,0754 & -0,7716 & 0,035 & 0,3393 & 0,0873 & 0,1992 \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & -5,8235 & -0,4752 & -25,682 & 0,6752 & 5,9191 & 0,2675 \\ \hline
				
				\makecell{Logistic \\ (Não Normalizada)} & 0,2611 & -0,606 & -27,619 & -0,4366 & -1,7861 & 0,3777 \\ \hline
			\end{tabular}
			\fonte{Próprio autor, 2018}
		\end{quadro}
		
		O comparativo dos dados existentes no quadro acima são vistas nos Gráficos de \ref{gra:comparativo1} à \ref{gra:comparativo4}. 
		
		\begin{grafico}
			\legenda[gra:comparativo1]{Resultados do comparativo para a função da tangente hiperbólica, base não normalizada e os valores únicos}
			\begin{figure}[H]
				\centering
				\fig{scale=0.9}{imagens/resultados_tanh_nnorm}\hfil
				\fonte{Próprio autor, 2018}
			\end{figure}
		\end{grafico}
		
		\begin{grafico}
			\legenda[gra:comparativo2]{Resultados do comparativo para a função da tangente hiperbólica, base normalizada e os valores únicos}
			\begin{figure}[H]
				\centering
				\fig{scale=0.9}{imagens/resultados_tanh_norm}\hfil
				\fonte{Próprio autor, 2018}
			\end{figure}
		\end{grafico}
	
		\begin{grafico}
			\legenda[gra:comparativo3]{Resultados do comparativo para a função logística, base não normalizada e os valores únicos}
			\begin{figure}[H]
				\centering
				\fig{scale=0.9}{imagens/resultados_logistic_nnorm}\hfil
				\fonte{Próprio autor, 2018}
			\end{figure}
		\end{grafico}
		
		\begin{grafico}
			\legenda[gra:comparativo4]{Resultados do comparativo para a função logística, base normalizada e os valores únicos}
			\begin{figure}[H]
				\centering
				\fig{scale=0.9}{imagens/resultados_logistic_norm}\hfil
				\fonte{Próprio autor, 2018}
			\end{figure}
		\end{grafico}
		
		Observando os gráficos acima, percebemos que a normalização ocasionou uma melhora no desempenho de aproximadamente 20\%. Além disso, observou-se que para as RNAs com a base normalizada, os parâmetros \textit{batch size}, número de ciclos, \textit{momentum} e número de neurônios tiveram uma maior influência. A substituição da função de ativação tangente hiperbólica pela logística ocasionou uma piora de aproximadamente 7,6\%, indicando que o uso da primeira permite um desempenho melhor.
		
		Analisando as RNAs com a base normalizada, percebe-se que a maioria dos parâmetros teve uma influência menor. Os parâmetros que tiveram uma influência positiva no desempenho foram o número de neurônios, taxa de aprendizado, número de ciclos e o número de camadas ocultas, que influenciou de forma insignificante e somente nas RNAs normalizadas com a função da tangente hiperbólica.
		\end{comment}
		%\postextual
		
		\chapter{Conclusão}
		\label{cap:conclusao}
			
		Analisando os resultados dos testes, observou-se que realizar o processo de normalização nos dados a serem processados pela Rede Neural Artificial fez com que, na maioria dos casos, o desempenho do treinamento das RNAs aumentasse de forma significativa. Outro fenômeno observado foi o fato da normalização diminuir a influência de alguns parâmetros utilizados nas RNAs, facilitando a sua configuração. 
		
		Com relação à taxa de aprendizado, um valor alto, juntamente com o uso de dados não normalizados fez com que o desempenho das RNAs caíssem, enquanto para dados normalizados, o desempenho aumentou. Para dados não normalizados, os melhores resultados foram obtidos com o uso do valor $0,1$ para dados não normalizados e valores entre $0,8$ e $1$ para dados normalizados.
		
		Com relação ao \textit{momentum}, um valor alto, independente da normalização dos dados fez com que o desempenho das RNAs caíssem. Os melhores resultados foram obtidos com o uso do valor $0,1$ para dados não normalizados e valores $0,8$, $0,9$ e $1$ para dados normalizados.
		
		Com relação ao número de camadas ocultas e ao número de neurônios em cada camada oculta, observou-se que adicionar camadas ocultas, independentemente do número de neurônios faz com que o desempenho caia utilizando a função de ativação logística, independentemente da normalização dos dados. No caso da função de ativação da tangente hiperbólica, por outro lado, teve um desempenho melhor, mantendo um aumento de desempenho até a 3ª camada oculta. Além disso, foi observado que das camadas ocultas testadas, a que teve maior influência no desempenho das Redes Neurais Artificiais foi a 1ª camada, seguido da 2ª camada oculta.\hspace{0.1cm}Já a adição de neurônios melhorou o desempenho da maioria das RNAs testadas. Isso indica que antes de adicionar uma nova camada oculta na RNA, deve-se experimentar aumentar o número de neurônios naquelas já existentes.
		
		O aumento do tamanho do lote (\textit{batch size}) ocasionou uma melhora significativa apenas nos casos sem a normalização dos dados, sendo os melhores parâmetros utilizados, os valores $468$ e $1875$ para dados não normalizados; $14$ e $29$ para dados normalizados. Já o aumento no número de ciclos permitiu uma melhora no desempenho, independente da função de ativação ou da normalização dos dados, sendo os melhores parâmetros utilizados, os valores entre $50$ e $500$ para dados não normalizados e entre $200$ e $1000$ para dados normalizados.
		
		As combinações mais interessantes utilizando os parâmetros testados foram:
		
		\begin{lista}
			\item Para dados não normalizados: A função logística se mostrou melhor nesses casos, juntamente com uma única camada oculta, um \textit{momentum} baixo, um tamanho do lote de médio a alto, uma taxa de aprendizado baixa e um número de ciclos médio. 
			
			\item Para dados normalizados: a função da tangente hiperbólica se mostrou melhor nesses casos, juntamente com um número de camadas ocultas entre 1 e 3, uma taxa de aprendizado alta, um valor de \textit{momentum} alto, um tamanho de lote baixo e um número de ciclos médio.
			
		\end{lista}
		
		Com o número crescente de estudos na área de \textit{Deep Learning}, pertencente a área de Aprendizado de Máquina e voltada para o desenvolvimento de soluções como o uso de RNAs profundas e suas variantes. As RNAs profundas tem como foco o uso de um grande número de camadas ocultas, tarefa que nenhuma das funções de ativação analisadas nesse trabalho são capazes de serem utilizadas com eficiência. Devido a isso, alguns trabalhos futuros sugeridos incluem a análise do uso de parâmetros de configurações próprias para o uso em RNAs profundas, criadas recentemente, tais como utilizar funções de ativação, por exemplo ReLU, ELU, ThresholdedReLU, PReLU, LeakyReLU e SELU, e configurá-las cada uma delas em camadas ocultas distintas. 

		Também é possível a utilização de outros algoritmos de otimização como RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam e TFOptimizer no lugar do algoritmo descida de gradiente, utilizado nesse trabalho. Outros parâmetros que também podem ser explorados são o uso de variações da configuração da taxa de aprendizado, como por exemplo configurá-la com decaimento durante o treinamento, o uso do \textit{alpha}, utilizado nesse trabalho, porém não analisado, e o uso de regularizadores L1 e L2, visando a redução do \textit{overfitting} conforme a adição de camadas ocultas.
		
		%Referências bibliográficas
		
		\bibliography{referencias}
			
		\end{document}